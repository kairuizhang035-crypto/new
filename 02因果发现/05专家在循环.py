#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
05 ä¸“å®¶åœ¨å¾ªç¯ (Expert In The Loop)
ä½¿ç”¨LLMè¿›è¡Œæ™ºèƒ½å› æœæ¨æ–­çš„å®Œæ•´ç‰ˆæœ¬

ä½œè€…: å› æœå‘ç°ç³»ç»Ÿ
æ—¥æœŸ: 2025å¹´
"""

from pgmpy.utils import get_example_model, llm_pairwise_orient
from pgmpy.estimators import ExpertInLoop, ExpertKnowledge
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import networkx as nx
import os
import json
from datetime import datetime
import warnings
from sklearn.exceptions import ConvergenceWarning
from sklearn.feature_selection import VarianceThreshold
from sklearn.preprocessing import StandardScaler
import re
from litellm import completion
import pgmpy.utils as pg_utils
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed

# è¿‡æ»¤è­¦å‘Š
warnings.filterwarnings('ignore', category=UserWarning, module='sklearn')
warnings.filterwarnings('ignore', category=RuntimeWarning, module='numpy')
warnings.filterwarnings('ignore', category=ConvergenceWarning)

os.environ.setdefault("OPENAI_BASE_URL", "http://localhost:11434/v1")
os.environ.setdefault("OPENAI_API_KEY", "EMPTY")
LLM_MODEL = os.environ.get("LLM_MODEL", "ollama/qwen2.5:7b")
MODEL_CHOICES = [
    "ollama/qwen2.5:7b",
    "ollama/qwen2.5:32b",
    "ollama/mannix/qwen2-57b:latest",
    "ollama/huihui_ai/deepseek-r1-abliterated:70b",
    "ollama/deepseek-r1:32b",
]
EIL_PVAL_THRESHOLD = float(os.environ.get("EIL_PVAL_THRESHOLD", "0.1"))
EIL_EFFECT_SIZE = float(os.environ.get("EIL_EFFECT_SIZE", "0.2"))
ORIENT_CACHE = {}
LLM_WORKERS = int(os.environ.get("LLM_WORKERS", "10"))
SELECT_PAIRS_THRESHOLD = float(os.environ.get("SELECT_PAIRS_THRESHOLD", "0.45"))
MAX_LLM_PAIRS = int(os.environ.get("MAX_LLM_PAIRS", "120"))
CANDIDATE_SET = set()
LLM_BATCH_SIZE = int(os.environ.get("LLM_BATCH_SIZE", "30"))
FAST_MODE = os.environ.get("FAST_MODE", "1") == "1"
LLM_BATCH_WORKERS = int(os.environ.get("LLM_BATCH_WORKERS", "10"))
SELECTED_PAIRS = []
EDGE_SOURCE = {}
LLM_ONLY = os.environ.get("LLM_ONLY", "1") == "1"

# è®¾ç½®ä¸­æ–‡å­—ä½“
import matplotlib
matplotlib.rcParams['font.family'] = ['sans-serif']
matplotlib.rcParams['font.sans-serif'] = [
    'SimHei', 'WenQuanYi Micro Hei', 'WenQuanYi Zen Hei', 
    'Noto Sans CJK SC', 'Source Han Sans SC', 'Microsoft YaHei',
    'DejaVu Sans', 'Arial Unicode MS', 'Liberation Sans'
]
matplotlib.rcParams['axes.unicode_minus'] = False

def load_data():
    """åŠ è½½æ•°æ®"""
    input_file = "/home/zkr/å› æœå‘ç°3/01æ•°æ®é¢„å¤„ç†/ç¼©å‡æ•°æ®_è§„æ ¼.csv"
    
    # å°è¯•ä½¿ç”¨utf-8ç¼–ç 
    try:
        df = pd.read_csv(input_file, encoding='utf-8', header=0, index_col=0)
    except UnicodeDecodeError:
        try:
            df = pd.read_csv(input_file, encoding='utf-8-sig', header=0, index_col=0)
        except UnicodeDecodeError:
            df = pd.read_csv(input_file, encoding='latin-1', header=0, index_col=0)
    
    df = df.dropna(axis=1, how='all')
    df = df.astype('float32')
    
    print(f"âœ“ æ•°æ®åŠ è½½å®Œæˆ: {df.shape}")
    return df

def create_output_folder():
    """åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹"""
    script_dir = os.path.dirname(os.path.abspath(__file__))
    output_dir = os.path.join(script_dir, "05ä¸“å®¶åœ¨å¾ªç¯ç»“æœ")
    os.makedirs(output_dir, exist_ok=True)
    return output_dir

def preprocess_data(df):
    """æ•°æ®é¢„å¤„ç†"""
    print("æ­£åœ¨è¿›è¡Œæ•°æ®è´¨é‡æ£€æŸ¥...")
    
    # å¤„ç†NaNå€¼
    if df.isnull().values.any():
        print("æ•°æ®ä¸­å­˜åœ¨ NaN å€¼ï¼Œä½¿ç”¨å‡å€¼å¡«å……")
        df = df.fillna(df.mean())
    
    # ç§»é™¤é›¶æ–¹å·®åˆ—
    zero_var_cols = df.columns[df.var() == 0]
    if not zero_var_cols.empty:
        print(f"ç§»é™¤é›¶æ–¹å·®åˆ—: {list(zero_var_cols)}")
        df = df.drop(columns=zero_var_cols)
    
    # å¤„ç†å¤šé‡å…±çº¿æ€§
    corr_matrix = df.corr().abs()
    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
    to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]
    
    if to_drop:
        df = df.drop(columns=to_drop)
        print(f"ç§»é™¤é«˜åº¦å…±çº¿åˆ—: {to_drop}")
    
    # æ–¹å·®é˜ˆå€¼è¿‡æ»¤
    selector = VarianceThreshold(threshold=0.01)
    df_transformed = selector.fit_transform(df)
    
    if df_transformed.shape[1] < df.shape[1]:
        retained_cols = df.columns[selector.get_support()]
        df = pd.DataFrame(df_transformed, columns=retained_cols, index=df.index)
        print(f"VarianceThresholdç§»é™¤äº† {df.shape[1] - df_transformed.shape[1]} ä¸ªä½æ–¹å·®åˆ—")
    
    print(f"âœ“ æ•°æ®é¢„å¤„ç†å®Œæˆï¼Œæœ€ç»ˆç»´åº¦: {df.shape}")
    return df

def create_variable_descriptions(df):
    """åˆ›å»ºå˜é‡æè¿°å­—å…¸"""
    variable_descriptions = {}
    for col in df.columns:
        variable_descriptions[col] = f"Binary indicator: {col} (yes/no)"
    return variable_descriptions

def robust_llm_orient(u, v, variable_descriptions=None, llm_model=LLM_MODEL, **kwargs):
    """ç¨³å¥çš„LLMå®šå‘å‡½æ•°"""
    if variable_descriptions is None:
        variable_descriptions = {}
    
    try:
        # ä½¿ç”¨åŸå§‹çš„LLMå®šå‘å‡½æ•°
        result = llm_pairwise_orient(u, v, variable_descriptions, llm_model)
        return result
    except Exception as e:
        print(f"LLMå®šå‘å¤±è´¥ ({u} <-> {v}): {e}")
        # ä½¿ç”¨å­—å…¸åºä½œä¸ºå›é€€
        return (u, v) if str(u) < str(v) else (v, u)

def _category(x):
    if isinstance(x, str):
        if x.startswith("ç–¾ç—…_"):
            return "ç–¾ç—…"
        if x.startswith("è¯ç‰©_"):
            return "è¯ç‰©"
        if x.startswith("æ£€éªŒ_"):
            return "æ£€éªŒ"
    return "å…¶ä»–"

def rule_based_orient(u, v):
    cu, cv = _category(u), _category(v)
    if cu == "è¯ç‰©" and cv == "æ£€éªŒ":
        return (u, v)
    if cu == "æ£€éªŒ" and cv == "è¯ç‰©":
        return (v, u)
    if cu == "ç–¾ç—…" and cv == "æ£€éªŒ":
        return (u, v)
    if cu == "æ£€éªŒ" and cv == "ç–¾ç—…":
        return (v, u)
    if cu == "ç–¾ç—…" and cv == "è¯ç‰©":
        return (u, v)
    if cu == "è¯ç‰©" and cv == "ç–¾ç—…":
        return (v, u)
    return None

def fast_llm_pairwise_orient(u, v, variable_descriptions=None, llm_model=LLM_MODEL):
    key = (str(u), str(v)) if str(u) < str(v) else (str(v), str(u))
    if key in ORIENT_CACHE:
        return ORIENT_CACHE[key]
    if key not in CANDIDATE_SET:
        rb = rule_based_orient(u, v)
        if rb is not None:
            ORIENT_CACHE[key] = rb
            EDGE_SOURCE[key] = "rule"
            return ORIENT_CACHE[key]
        ORIENT_CACHE[key] = (u, v) if str(u) < str(v) else (v, u)
        EDGE_SOURCE[key] = "fallback"
        return ORIENT_CACHE[key]
    sys_msg = {
        "role": "system",
        "content": "åªè¾“å‡ºä¸€è¡Œï¼Œæ ¼å¼ä¸¥æ ¼ä¸º 'A->B' æˆ– 'B->A'ï¼Œä¸æ·»åŠ ä»»ä½•è§£é‡Šã€‚"
    }
    desc_u = variable_descriptions.get(u, str(u)) if variable_descriptions else str(u)
    desc_v = variable_descriptions.get(v, str(v)) if variable_descriptions else str(v)
    user_msg = {
        "role": "user",
        "content": f"å˜é‡A: {u}ï¼›æè¿°: {desc_u}\nå˜é‡B: {v}ï¼›æè¿°: {desc_v}\nä»…è¾“å‡ºä¸€ä¸ªç»“æœ: '{u}->{v}' æˆ– '{v}->{u}'ã€‚"
    }
    try:
        resp = completion(model=llm_model, messages=[sys_msg, user_msg], temperature=0, max_tokens=16, timeout=15)
        text = resp["choices"][0]["message"]["content"].strip()
        p1 = rf"{re.escape(str(u))}\s*(?:->|=>|â†’)\s*{re.escape(str(v))}"
        p2 = rf"{re.escape(str(v))}\s*(?:->|=>|â†’)\s*{re.escape(str(u))}"
        if re.search(p1, text, re.IGNORECASE):
            ORIENT_CACHE[key] = (u, v)
            EDGE_SOURCE[key] = "llm"
            return ORIENT_CACHE[key]
        if re.search(p2, text, re.IGNORECASE):
            ORIENT_CACHE[key] = (v, u)
            EDGE_SOURCE[key] = "llm"
            return ORIENT_CACHE[key]
        rb = rule_based_orient(u, v)
        if rb is not None:
            ORIENT_CACHE[key] = rb
            EDGE_SOURCE[key] = "rule"
            return ORIENT_CACHE[key]
        ORIENT_CACHE[key] = (u, v) if str(u) < str(v) else (v, u)
        EDGE_SOURCE[key] = "fallback"
        return ORIENT_CACHE[key]
    except Exception:
        rb = rule_based_orient(u, v)
        if rb is not None:
            ORIENT_CACHE[key] = rb
            EDGE_SOURCE[key] = "rule"
            return ORIENT_CACHE[key]
        ORIENT_CACHE[key] = (u, v) if str(u) < str(v) else (v, u)
        EDGE_SOURCE[key] = "fallback"
        return ORIENT_CACHE[key]

def batched_llm_orient(pairs, variable_descriptions=None, llm_model=LLM_MODEL):
    lines = []
    for u, v in pairs:
        du = variable_descriptions.get(u, str(u)) if variable_descriptions else str(u)
        dv = variable_descriptions.get(v, str(v)) if variable_descriptions else str(v)
        lines.append(f"A={u};desc={du} | B={v};desc={dv}")
    sys_msg = {"role": "system", "content": "ä»…è¾“å‡ºè‹¥å¹²è¡Œï¼Œæ¯è¡Œæ ¼å¼ 'X->Y' æˆ– 'Y->X'ï¼Œä¸å«å…¶å®ƒå†…å®¹ã€‚"}
    user_msg = {"role": "user", "content": "\n".join(lines)}
    try:
        resp = completion(model=llm_model, messages=[sys_msg, user_msg], temperature=0, max_tokens=LLM_BATCH_SIZE*16, timeout=45)
        text = resp["choices"][0]["message"]["content"].strip()
        outs = [t.strip() for t in text.splitlines() if t.strip()]
        assigned = set()
        for out in outs:
            for u, v in pairs:
                k = (str(u), str(v)) if str(u) < str(v) else (str(v), str(u))
                if k in assigned:
                    continue
                p1 = rf"{re.escape(str(u))}\s*(?:->|=>|â†’)\s*{re.escape(str(v))}"
                p2 = rf"{re.escape(str(v))}\s*(?:->|=>|â†’)\s*{re.escape(str(u))}"
                if re.search(p1, out, re.IGNORECASE):
                    ORIENT_CACHE[k] = (u, v)
                    EDGE_SOURCE[k] = "llm"
                    assigned.add(k)
                elif re.search(p2, out, re.IGNORECASE):
                    ORIENT_CACHE[k] = (v, u)
                    EDGE_SOURCE[k] = "llm"
                    assigned.add(k)
        for u, v in pairs:
            k = (str(u), str(v)) if str(u) < str(v) else (str(v), str(u))
            if k in assigned:
                continue
            rb = rule_based_orient(u, v)
            if rb is not None:
                ORIENT_CACHE[k] = rb
                EDGE_SOURCE[k] = "rule"
            else:
                ORIENT_CACHE[k] = (u, v) if str(u) < str(v) else (v, u)
                EDGE_SOURCE[k] = "fallback"
    except Exception:
        for u, v in pairs:
            k = (str(u), str(v)) if str(u) < str(v) else (str(v), str(u))
            rb = rule_based_orient(u, v)
            if rb is not None:
                ORIENT_CACHE[k] = rb
                EDGE_SOURCE[k] = "rule"
            else:
                ORIENT_CACHE[k] = (u, v) if str(u) < str(v) else (v, u)
                EDGE_SOURCE[k] = "fallback"

def precompute_orientations(df, variable_descriptions):
    corr = df.corr().abs()
    pairs = []
    cols = list(df.columns)
    for i in range(len(cols)):
        for j in range(i+1, len(cols)):
            s = corr.loc[cols[i], cols[j]]
            if s >= SELECT_PAIRS_THRESHOLD:
                pairs.append((cols[i], cols[j], float(s)))
    pairs.sort(key=lambda x: x[2], reverse=True)
    global SELECTED_PAIRS
    SELECTED_PAIRS = [(u, v) for u, v, _ in pairs[:MAX_LLM_PAIRS]]
    for u, v in SELECTED_PAIRS:
        k = (str(u), str(v)) if str(u) < str(v) else (str(v), str(u))
        CANDIDATE_SET.add(k)
    raw_count = len(pairs)
    if SELECTED_PAIRS:
        chunks = [SELECTED_PAIRS[i:i+LLM_BATCH_SIZE] for i in range(0, len(SELECTED_PAIRS), LLM_BATCH_SIZE)]
        total = len(chunks)
        print(f"ç›¸å…³é˜ˆå€¼{SELECT_PAIRS_THRESHOLD}ä¸‹å…±{raw_count}å¯¹ï¼›è¿›å…¥LLMå®šå‘{len(SELECTED_PAIRS)}å¯¹ï¼ˆä¸Šé™{MAX_LLM_PAIRS}ï¼‰")
        print(f"é¢„å®šå‘æ‰¹æ¬¡: {total}ï¼Œå€™é€‰å¯¹æ•°: {len(SELECTED_PAIRS)}")
        done = 0
        with ThreadPoolExecutor(max_workers=LLM_BATCH_WORKERS) as ex:
            futs = [ex.submit(batched_llm_orient, ch, variable_descriptions, LLM_MODEL) for ch in chunks]
            for _ in as_completed(futs):
                done += 1
                if done % 2 == 0 or done == total:
                    print(f"é¢„å®šå‘è¿›åº¦: {done}/{total}")

def save_dag_results(dag, output_folder, df_columns):
    """ä¿å­˜DAGç»“æœåˆ°æ–‡ä»¶"""
    edges = list(dag.edges())
    
    # ä¿å­˜TXTæ ¼å¼
    txt_file = os.path.join(output_folder, "ExpertInLoop_å› æœè¾¹å®Œæ•´.txt")
    with open(txt_file, 'w', encoding='utf-8') as f:
        f.write("ä¸“å®¶åœ¨å¾ªç¯ (Expert In The Loop) å‘ç°çš„å› æœè¾¹\n")
        f.write("=" * 50 + "\n")
        for i, edge in enumerate(edges, 1):
            f.write(f"{i:3d}. {edge[0]} -> {edge[1]}\n")
    
    # ä¿å­˜CSVæ ¼å¼
    df_edges = pd.DataFrame(edges, columns=["æºèŠ‚ç‚¹", "ç›®æ ‡èŠ‚ç‚¹"])
    csv_file = os.path.join(output_folder, "ExpertInLoop_å› æœè¾¹åˆ—è¡¨.csv")
    df_edges.to_csv(csv_file, index=False, encoding="utf-8-sig")
    
    # ç”Ÿæˆç½‘ç»œå›¾
    plt.figure(figsize=(16, 12))
    G = nx.DiGraph()
    G.add_edges_from(edges)
    
    if len(edges) > 0:
        pos = nx.spring_layout(G, k=3, iterations=50, seed=42)
        
        # ç»˜åˆ¶èŠ‚ç‚¹
        nx.draw_networkx_nodes(G, pos, 
                              node_color='lightpink', 
                              node_size=2000,
                              alpha=0.8)
        
        # ç»˜åˆ¶è¾¹
        nx.draw_networkx_edges(G, pos, 
                              edge_color='gray',
                              arrows=True,
                              arrowsize=20,
                              arrowstyle='->',
                              width=1.5,
                              alpha=0.7)
        
        # ç»˜åˆ¶æ ‡ç­¾
        nx.draw_networkx_labels(G, pos, 
                               font_size=10,
                               font_weight='bold',
                               font_family='sans-serif')
    
    plt.title(f"ä¸“å®¶åœ¨å¾ªç¯ (Expert In The Loop) å› æœç½‘ç»œå›¾\nå…±{len(edges)}æ¡å› æœè¾¹", 
              fontsize=16, fontweight='bold')
    plt.axis('off')
    plt.tight_layout()
    
    graph_file = os.path.join(output_folder, "ExpertInLoop_å› æœç½‘ç»œå›¾.png")
    plt.savefig(graph_file, dpi=300, bbox_inches='tight', 
                facecolor='white', edgecolor='none')
    plt.close()
    
    # åˆ›å»ºè¯¦ç»†JSONç»“æœ
    G = nx.DiGraph()
    G.add_edges_from(edges)
    
    in_degrees = dict(G.in_degree())
    out_degrees = dict(G.out_degree())
    
    results = {
        "ç®—æ³•ä¿¡æ¯": {
            "ç®—æ³•åç§°": "ä¸“å®¶åœ¨å¾ªç¯ (Expert In The Loop)",
            "ç­–ç•¥": "LLMæ™ºèƒ½å®šå‘",
            "ç”Ÿæˆæ—¶é—´": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "æ•°æ®ç»´åº¦": {
                "æ ·æœ¬æ•°": len(df_columns),
                "å˜é‡æ•°": len(df_columns)
            }
        },
        "ç½‘ç»œç»“æ„": {
            "èŠ‚ç‚¹æ€»æ•°": len(dag.nodes()),
            "è¾¹æ€»æ•°": len(edges),
            "èŠ‚ç‚¹åˆ—è¡¨": list(dag.nodes()),
            "å› æœè¾¹åˆ—è¡¨": [{"æºèŠ‚ç‚¹": edge[0], "ç›®æ ‡èŠ‚ç‚¹": edge[1]} for edge in edges]
        },
        "ç»Ÿè®¡ä¿¡æ¯": {
            "å…¥åº¦ç»Ÿè®¡": {node: in_degrees.get(node, 0) for node in dag.nodes()},
            "å‡ºåº¦ç»Ÿè®¡": {node: out_degrees.get(node, 0) for node in dag.nodes()},
            "æœ€å¤§å…¥åº¦": max(in_degrees.values()) if in_degrees else 0,
            "æœ€å¤§å‡ºåº¦": max(out_degrees.values()) if out_degrees else 0,
            "å¹³å‡åº¦æ•°": sum(dict(G.degree()).values()) / len(dag.nodes()) if dag.nodes() else 0
        },
        "èŠ‚ç‚¹åˆ†æ": {
            "æ ¹èŠ‚ç‚¹": [node for node in dag.nodes() if in_degrees.get(node, 0) == 0],
            "å¶èŠ‚ç‚¹": [node for node in dag.nodes() if out_degrees.get(node, 0) == 0],
            "ä¸­ä»‹èŠ‚ç‚¹": [node for node in dag.nodes() if in_degrees.get(node, 0) > 0 and out_degrees.get(node, 0) > 0]
        }
    }
    
    json_file = os.path.join(output_folder, "ExpertInLoop_å› æœç»“æœ.json")
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    return txt_file, csv_file, graph_file, json_file, results

def run_expert_in_loop_algorithm():
    global LLM_MODEL, SELECT_PAIRS_THRESHOLD, MAX_LLM_PAIRS, LLM_ONLY
    """è¿è¡Œä¸“å®¶åœ¨å¾ªç¯ç®—æ³•"""
    print("=" * 60)
    print("05 ä¸“å®¶åœ¨å¾ªç¯ (Expert In The Loop) - å¼€å§‹æ‰§è¡Œ")
    print("ä½¿ç”¨LLMè¿›è¡Œæ™ºèƒ½å› æœæ¨æ–­")
    print("=" * 60)
    try:
        print("æ¨¡å‹é€‰æ‹©: 0:qwen2.5:7b  1:qwen2.5:32b  2:mannix/qwen2-57b:latest  3:huihui_ai/deepseek-r1-abliterated:70b  4:deepseek-r1:32b")
        s = input("è¯·é€‰æ‹©æ¨¡å‹ç¼–å·(é»˜è®¤0): ").strip()
        idx = 0 if s == "" else int(s)
        if 0 <= idx < len(MODEL_CHOICES):
            LLM_MODEL = MODEL_CHOICES[idx]
            os.environ["LLM_MODEL"] = LLM_MODEL
            print(f"ä½¿ç”¨æ¨¡å‹: {LLM_MODEL}")
        t = input(f"å€™é€‰ç­›é€‰é˜ˆå€¼(å½“å‰{SELECT_PAIRS_THRESHOLD}): ").strip()
        if t:
            try:
                v = float(t)
                SELECT_PAIRS_THRESHOLD = v
                os.environ["SELECT_PAIRS_THRESHOLD"] = str(v)
                print(f"è®¾å®šé˜ˆå€¼ä¸º: {SELECT_PAIRS_THRESHOLD}")
            except:
                pass
        m = input(f"å€™é€‰æ•°é‡ä¸Šé™(å½“å‰{MAX_LLM_PAIRS}): ").strip()
        if m:
            try:
                mv = int(m)
                MAX_LLM_PAIRS = mv
                os.environ["MAX_LLM_PAIRS"] = str(mv)
                print(f"è®¾å®šä¸Šé™ä¸º: {MAX_LLM_PAIRS}")
            except:
                pass
        yn = input(f"ä»…æ·»åŠ æ¥æºä¸ºLLMçš„è¾¹(å½“å‰{'æ˜¯' if LLM_ONLY else 'å¦'}) [Y/n]: ").strip().lower()
        if yn in ("", "y", "yes"):
            LLM_ONLY = True
            os.environ["LLM_ONLY"] = "1"
        elif yn in ("n", "no"):
            LLM_ONLY = False
            os.environ["LLM_ONLY"] = "0"
        print(f"ä»…æ·»åŠ LLMè¾¹: {'æ˜¯' if LLM_ONLY else 'å¦'}")
    except Exception:
        pass
    
    # 1. åŠ è½½æ•°æ®
    df = load_data()
    
    # 2. åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹
    output_dir = create_output_folder()
    
    # 3. æ•°æ®é¢„å¤„ç†
    df_processed = preprocess_data(df)
    
    # 4. åˆ›å»ºå˜é‡æè¿°
    variable_descriptions = create_variable_descriptions(df_processed)
    print(f"âœ“ åˆ›å»ºäº†{len(variable_descriptions)}ä¸ªå˜é‡çš„æè¿°")
    
    # 5. ä½¿ç”¨Expert-in-the-Loopè¿›è¡Œå› æœå‘ç°
    print("ä½¿ç”¨Expert-in-the-Loopæ–¹æ³•ï¼Œç»“åˆLLMè¿›è¡Œè¾¹å®šå‘...")
    start_time = time.time()
    
    try:
        # åˆ›å»ºExpertInLoopä¼°è®¡å™¨
        precompute_orientations(df_processed, variable_descriptions)
        pg_utils.llm_pairwise_orient = fast_llm_pairwise_orient
        if FAST_MODE:
            from pgmpy.base import DAG
            dag = DAG()
            dag.add_nodes_from(df_processed.columns)
            processed = 0
            total_pairs = len(SELECTED_PAIRS)
            for u, v in SELECTED_PAIRS:
                k = (str(u), str(v)) if str(u) < str(v) else (str(v), str(u))
                o = ORIENT_CACHE.get(k)
                o = o if o is not None else fast_llm_pairwise_orient(u, v, variable_descriptions, LLM_MODEL)
                src = EDGE_SOURCE.get(k, "unknown")
                if LLM_ONLY and src != "llm":
                    processed += 1
                    if processed % 20 == 0 or processed == total_pairs:
                        print(f"æ„å›¾è¿›åº¦: å·²å¤„ç†å€™é€‰å¯¹ {processed}/{total_pairs}")
                    continue
                try:
                    dag.add_edge(o[0], o[1])
                except:
                    pass
                processed += 1
                if processed % 20 == 0 or processed == total_pairs:
                    print(f"æ„å›¾è¿›åº¦: å·²å¤„ç†å€™é€‰å¯¹ {processed}/{total_pairs}")
            keys = [(str(u), str(v)) if str(u) < str(v) else (str(v), str(u)) for u, v in SELECTED_PAIRS]
            c_llm = sum(1 for k in keys if EDGE_SOURCE.get(k) == "llm")
            c_rule = sum(1 for k in keys if EDGE_SOURCE.get(k) == "rule")
            c_fb = sum(1 for k in keys if EDGE_SOURCE.get(k) == "fallback")
            print(f"å®šå‘æ¥æº: LLM={c_llm} è§„åˆ™={c_rule} å›é€€={c_fb}")
            learned_dag = dag
        else:
            estimator = ExpertInLoop(df_processed)
            learned_dag = estimator.estimate(
                pval_threshold=EIL_PVAL_THRESHOLD,
                effect_size_threshold=EIL_EFFECT_SIZE,
                variable_descriptions=variable_descriptions,
                llm_model=LLM_MODEL,
                use_cache=True,
                show_progress=True
            )
        
        if learned_dag is None:
            raise ValueError("ExpertInLoop.estimate() è¿”å›äº† None")
        
        end_time = time.time()
        execution_time = end_time - start_time
        
        print(f"âœ“ ä¸“å®¶åœ¨å¾ªç¯å®Œæˆï¼Œè€—æ—¶: {execution_time:.2f}ç§’")
        print(f"âœ“ å‘ç° {len(learned_dag.edges())} æ¡å› æœè¾¹")
        
        # 6. ä¿å­˜ç»“æœ
        txt_file, csv_file, graph_file, json_file, results = save_dag_results(learned_dag, output_dir, df_processed.columns)
        
        # 7. è¾“å‡ºç»“æœæ‘˜è¦
        print("\n" + "=" * 60)
        print("ä¸“å®¶åœ¨å¾ªç¯æ‰§è¡Œå®Œæˆ - ç»“æœæ‘˜è¦")
        print("=" * 60)
        print(f"ç­–ç•¥: LLMæ™ºèƒ½å®šå‘")
        print(f"æ‰§è¡Œæ—¶é—´: {execution_time:.2f}ç§’")
        print(f"æ•°æ®ç»´åº¦: {df_processed.shape[0]} Ã— {df_processed.shape[1]}")
        print(f"å‘ç°çš„å› æœè¾¹æ•°é‡: {results['ç½‘ç»œç»“æ„']['è¾¹æ€»æ•°']}")
        print(f"ç½‘ç»œèŠ‚ç‚¹æ•°é‡: {results['ç½‘ç»œç»“æ„']['èŠ‚ç‚¹æ€»æ•°']}")
        print(f"æ ¹èŠ‚ç‚¹æ•°é‡: {len(results['èŠ‚ç‚¹åˆ†æ']['æ ¹èŠ‚ç‚¹'])}")
        print(f"å¶èŠ‚ç‚¹æ•°é‡: {len(results['èŠ‚ç‚¹åˆ†æ']['å¶èŠ‚ç‚¹'])}")
        print(f"ä¸­ä»‹èŠ‚ç‚¹æ•°é‡: {len(results['èŠ‚ç‚¹åˆ†æ']['ä¸­ä»‹èŠ‚ç‚¹'])}")
        print(f"å¹³å‡èŠ‚ç‚¹åº¦æ•°: {results['ç»Ÿè®¡ä¿¡æ¯']['å¹³å‡åº¦æ•°']:.2f}")
        
        print(f"\nğŸ“ ç»“æœä¿å­˜ä½ç½®:")
        print(f"  - TXTæ–‡ä»¶: {txt_file}")
        print(f"  - CSVæ–‡ä»¶: {csv_file}")
        print(f"  - ç½‘ç»œå›¾: {graph_file}")
        print(f"  - JSONç»“æœ: {json_file}")
        
        return output_dir, len(learned_dag.edges())
        
    except Exception as e:
        print(f"âŒ ä¸“å®¶åœ¨å¾ªç¯æ‰§è¡Œå¤±è´¥: {str(e)}")
        # ä½¿ç”¨å¿«é€Ÿå›é€€ç­–ç•¥
        print("ä½¿ç”¨å¿«é€Ÿå›é€€ç­–ç•¥...")
        from pgmpy.base import DAG
        
        dag = DAG()
        dag.add_nodes_from(df_processed.columns)
        
        # åŸºäºç›¸å…³æ€§æ·»åŠ è¾¹
        corr_matrix = df_processed.corr().abs()
        edges_added = 0
        max_edges = 50
        
        for i, col1 in enumerate(df_processed.columns):
            for j, col2 in enumerate(df_processed.columns):
                if i < j and edges_added < max_edges:
                    corr_val = corr_matrix.loc[col1, col2]
                    if corr_val >= 0.3:
                        try:
                            dag.add_edge(col1, col2)
                            edges_added += 1
                        except:
                            continue
        
        end_time = time.time()
        execution_time = end_time - start_time
        
        print(f"âœ“ å¿«é€Ÿå›é€€å®Œæˆï¼Œè€—æ—¶: {execution_time:.2f}ç§’")
        print(f"âœ“ å‘ç° {len(dag.edges())} æ¡å› æœè¾¹")
        
        txt_file, csv_file, graph_file, json_file, results = save_dag_results(dag, output_dir, df_processed.columns)
        
        return output_dir, len(dag.edges())

if __name__ == "__main__":
    import time
    try:
        output_dir, edge_count = run_expert_in_loop_algorithm()
        print(f"\nâœ… 05 ä¸“å®¶åœ¨å¾ªç¯æ‰§è¡ŒæˆåŠŸï¼å‘ç° {edge_count} æ¡å› æœè¾¹")
    except Exception as e:
        print(f"\nâŒ 05 ä¸“å®¶åœ¨å¾ªç¯æ‰§è¡Œå¤±è´¥: {str(e)}")
        raise
