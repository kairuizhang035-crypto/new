# å› æžœå‘çŽ° 1-6 è„šæœ¬æ±‡ç¼–

## 01 PCç®—æ³•

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
01 PCç®—æ³• (åŸºäºŽçº¦æŸçš„ä¼°è®¡å™¨)
éžäº¤äº’å¼ç‰ˆæœ¬ï¼Œç”¨äºŽç»Ÿä¸€æ‰§è¡Œæµç¨‹

ä½œè€…: å› æžœå‘çŽ°ç³»ç»Ÿ
æ—¥æœŸ: 2025å¹´
"""

import pandas as pd
import os
import json
import matplotlib.pyplot as plt
import networkx as nx
from datetime import datetime
import numpy as np
from pgmpy.estimators import PC

# è®¾ç½®ä¸­æ–‡å­—ä½“
import matplotlib
matplotlib.rcParams['font.family'] = ['sans-serif']
matplotlib.rcParams['font.sans-serif'] = [
    'SimHei', 'WenQuanYi Micro Hei', 'WenQuanYi Zen Hei', 
    'Noto Sans CJK SC', 'Source Han Sans SC', 'Microsoft YaHei',
    'DejaVu Sans', 'Arial Unicode MS', 'Liberation Sans'
]
matplotlib.rcParams['axes.unicode_minus'] = False

def load_data():
    """åŠ è½½æ•°æ®"""
    input_file = "/home/zkr/å› æžœå‘çŽ°3/01æ•°æ®é¢„å¤„ç†/ç¼©å‡æ•°æ®_è§„æ ¼.csv"
    
    # å°è¯•ä½¿ç”¨utf-8ç¼–ç 
    try:
        df = pd.read_csv(input_file, encoding='utf-8', header=0, index_col=0)
    except UnicodeDecodeError:
        try:
            df = pd.read_csv(input_file, encoding='utf-8-sig', header=0, index_col=0)
        except UnicodeDecodeError:
            df = pd.read_csv(input_file, encoding='latin-1', header=0, index_col=0)
    
    df = df.dropna(axis=1, how='all')
    df = df.astype('float32')
    
    print(f"âœ“ æ•°æ®åŠ è½½å®Œæˆ: {df.shape}")
    return df

def create_output_folder():
    base_root = "/home/zkr/å› æžœå‘çŽ°3/02å› æžœå‘çŽ°"
    output_dir = os.path.join(base_root, "01PCç®—æ³•ç»“æžœ")
    os.makedirs(output_dir, exist_ok=True)
    return output_dir

def create_causal_network_graph(edges, output_dir):
    """åˆ›å»ºå› æžœç½‘ç»œå›¾å¹¶ä¿å­˜ä¸ºPNG"""
    G = nx.DiGraph()
    
    # æ·»åŠ è¾¹
    for edge in edges:
        G.add_edge(edge[0], edge[1])
    
    plt.figure(figsize=(16, 12))
    
    # ä½¿ç”¨springå¸ƒå±€
    pos = nx.spring_layout(G, k=3, iterations=50, seed=42)
    
    # ç»˜åˆ¶èŠ‚ç‚¹
    nx.draw_networkx_nodes(G, pos, 
                          node_color='lightblue', 
                          node_size=2000,
                          alpha=0.8)
    
    # ç»˜åˆ¶è¾¹
    nx.draw_networkx_edges(G, pos, 
                          edge_color='gray',
                          arrows=True,
                          arrowsize=20,
                          arrowstyle='->',
                          width=1.5,
                          alpha=0.7)
    
    # ç»˜åˆ¶æ ‡ç­¾
    nx.draw_networkx_labels(G, pos, 
                           font_size=10,
                           font_weight='bold',
                           font_family='sans-serif')
    
    plt.title(f"PCç®—æ³•å› æžœç½‘ç»œå›¾\nå…±{len(edges)}æ¡å› æžœè¾¹", fontsize=16, fontweight='bold')
    plt.axis('off')
    plt.tight_layout()
    
    graph_file = os.path.join(output_dir, "PC_å› æžœç½‘ç»œå›¾.png")
    plt.savefig(graph_file, dpi=300, bbox_inches='tight', 
                facecolor='white', edgecolor='none')
    plt.close()
    
    return graph_file

def create_detailed_json_results(estimated_model, df, output_dir):
    """åˆ›å»ºè¯¦ç»†çš„å› æžœå‘çŽ°ç»“æžœJSONæ–‡ä»¶"""
    nodes = list(estimated_model.nodes())
    edges = list(estimated_model.edges())
    
    # è®¡ç®—ç½‘ç»œç»Ÿè®¡ä¿¡æ¯
    G = nx.DiGraph()
    G.add_edges_from(edges)
    
    # èŠ‚ç‚¹åº¦æ•°ç»Ÿè®¡
    in_degrees = dict(G.in_degree())
    out_degrees = dict(G.out_degree())
    
    # åˆ›å»ºè¯¦ç»†ç»“æžœå­—å…¸
    results = {
        "ç®—æ³•ä¿¡æ¯": {
            "ç®—æ³•åç§°": "PCç®—æ³•",
            "ç”Ÿæˆæ—¶é—´": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "æ•°æ®ç»´åº¦": {
                "æ ·æœ¬æ•°": int(df.shape[0]),
                "å˜é‡æ•°": int(df.shape[1])
            },
            "å‚æ•°è®¾ç½®": {
                "ç‹¬ç«‹æ€§æ£€éªŒ": "chi_square",
                "æ˜¾è‘—æ€§æ°´å¹³": 0.05,
                "å˜ä½“": "stable"
            }
        },
        "ç½‘ç»œç»“æž„": {
            "èŠ‚ç‚¹æ€»æ•°": len(nodes),
            "è¾¹æ€»æ•°": len(edges),
            "èŠ‚ç‚¹åˆ—è¡¨": nodes,
            "å› æžœè¾¹åˆ—è¡¨": [{"æºèŠ‚ç‚¹": edge[0], "ç›®æ ‡èŠ‚ç‚¹": edge[1]} for edge in edges]
        },
        "ç»Ÿè®¡ä¿¡æ¯": {
            "å…¥åº¦ç»Ÿè®¡": {node: in_degrees.get(node, 0) for node in nodes},
            "å‡ºåº¦ç»Ÿè®¡": {node: out_degrees.get(node, 0) for node in nodes},
            "æœ€å¤§å…¥åº¦": max(in_degrees.values()) if in_degrees else 0,
            "æœ€å¤§å‡ºåº¦": max(out_degrees.values()) if out_degrees else 0,
            "å¹³å‡åº¦æ•°": sum(dict(G.degree()).values()) / len(nodes) if nodes else 0
        },
        "èŠ‚ç‚¹åˆ†æž": {
            "æ ¹èŠ‚ç‚¹": [node for node in nodes if in_degrees.get(node, 0) == 0],
            "å¶èŠ‚ç‚¹": [node for node in nodes if out_degrees.get(node, 0) == 0],
            "ä¸­ä»‹èŠ‚ç‚¹": [node for node in nodes if in_degrees.get(node, 0) > 0 and out_degrees.get(node, 0) > 0]
        }
    }
    
    # ä¿å­˜JSONæ–‡ä»¶
    json_file = os.path.join(output_dir, "PC_å› æžœç»“æžœ.json")
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    return json_file, results

def run_pc_algorithm():
    """è¿è¡ŒPCç®—æ³•"""
    print("=" * 60)
    print("01 PCç®—æ³• (åŸºäºŽçº¦æŸçš„ä¼°è®¡å™¨) - å¼€å§‹æ‰§è¡Œ")
    print("=" * 60)
    
    # 1. åŠ è½½æ•°æ®
    df = load_data()
    
    # 2. åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹
    output_dir = create_output_folder()
    
    # 3. åˆå§‹åŒ–PCç®—æ³•ä¼°è®¡å™¨
    print("æ­£åœ¨è¿è¡ŒPCç®—æ³•...")
    est = PC(data=df)
    
    # 4. è¿è¡Œä¼°è®¡ç®—æ³•
    estimated_model = est.estimate(variant="stable", ci_test="chi_square", significance_level=0.05)
    
    # 5. èŽ·å–ç»“æžœ
    edges_list = list(estimated_model.edges())
    print(f"âœ“ PCç®—æ³•å®Œæˆï¼Œå‘çŽ° {len(edges_list)} æ¡å› æžœè¾¹")
    
    # 6. ä¿å­˜ç»“æžœæ–‡ä»¶
    # ä¿å­˜TXTæ ¼å¼
    output_file_txt = os.path.join(output_dir, "PC_å› æžœè¾¹å®Œæ•´.txt")
    with open(output_file_txt, 'w', encoding='utf-8') as f:
        f.write("PCç®—æ³•å‘çŽ°çš„å› æžœè¾¹\n")
        f.write("=" * 30 + "\n")
        for i, edge in enumerate(edges_list, 1):
            f.write(f"{i:3d}. {edge[0]} -> {edge[1]}\n")
    
    # ä¿å­˜CSVæ ¼å¼
    df_edges = pd.DataFrame(edges_list, columns=["æºèŠ‚ç‚¹", "ç›®æ ‡èŠ‚ç‚¹"])
    output_file_csv = os.path.join(output_dir, "PC_å› æžœè¾¹åˆ—è¡¨.csv")
    df_edges.to_csv(output_file_csv, index=False, encoding="utf-8-sig")
    
    # 7. ç”Ÿæˆç½‘ç»œå›¾
    print("æ­£åœ¨ç”Ÿæˆå› æžœç½‘ç»œå›¾...")
    graph_file = create_causal_network_graph(edges_list, output_dir)
    
    # 8. ç”ŸæˆJSONç»“æžœ
    print("æ­£åœ¨ç”Ÿæˆè¯¦ç»†JSONç»“æžœ...")
    json_file, results = create_detailed_json_results(estimated_model, df, output_dir)
    
    # 9. è¾“å‡ºç»“æžœæ‘˜è¦
    print("\n" + "=" * 60)
    print("PCç®—æ³•æ‰§è¡Œå®Œæˆ - ç»“æžœæ‘˜è¦")
    print("=" * 60)
    print(f"æ•°æ®ç»´åº¦: {results['ç®—æ³•ä¿¡æ¯']['æ•°æ®ç»´åº¦']['æ ·æœ¬æ•°']} Ã— {results['ç®—æ³•ä¿¡æ¯']['æ•°æ®ç»´åº¦']['å˜é‡æ•°']}")
    print(f"å‘çŽ°çš„å› æžœè¾¹æ•°é‡: {results['ç½‘ç»œç»“æž„']['è¾¹æ€»æ•°']}")
    print(f"ç½‘ç»œèŠ‚ç‚¹æ•°é‡: {results['ç½‘ç»œç»“æž„']['èŠ‚ç‚¹æ€»æ•°']}")
    print(f"æ ¹èŠ‚ç‚¹æ•°é‡: {len(results['èŠ‚ç‚¹åˆ†æž']['æ ¹èŠ‚ç‚¹'])}")
    print(f"å¶èŠ‚ç‚¹æ•°é‡: {len(results['èŠ‚ç‚¹åˆ†æž']['å¶èŠ‚ç‚¹'])}")
    print(f"ä¸­ä»‹èŠ‚ç‚¹æ•°é‡: {len(results['èŠ‚ç‚¹åˆ†æž']['ä¸­ä»‹èŠ‚ç‚¹'])}")
    print(f"å¹³å‡èŠ‚ç‚¹åº¦æ•°: {results['ç»Ÿè®¡ä¿¡æ¯']['å¹³å‡åº¦æ•°']:.2f}")
    
    print(f"\nðŸ“ ç»“æžœä¿å­˜ä½ç½®:")
    print(f"  - TXTæ–‡ä»¶: {output_file_txt}")
    print(f"  - CSVæ–‡ä»¶: {output_file_csv}")
    print(f"  - ç½‘ç»œå›¾: {graph_file}")
    print(f"  - JSONç»“æžœ: {json_file}")
    
    return output_dir, len(edges_list)

if __name__ == "__main__":
    try:
        output_dir, edge_count = run_pc_algorithm()
        print(f"\nâœ… 01 PCç®—æ³•æ‰§è¡ŒæˆåŠŸï¼å‘çŽ° {edge_count} æ¡å› æžœè¾¹")
    except Exception as e:
        print(f"\nâŒ 01 PCç®—æ³•æ‰§è¡Œå¤±è´¥: {str(e)}")
        raise
```

## 02 çˆ¬å±±ç®—æ³• (Hill Climbing)

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
02 çˆ¬å±±ç®—æ³• (Hill Climbing Search)
éžäº¤äº’å¼ç‰ˆæœ¬ï¼Œä½¿ç”¨AIC-Dè¯„åˆ†æ ‡å‡†

ä½œè€…: å› æžœå‘çŽ°ç³»ç»Ÿ
æ—¥æœŸ: 2025å¹´
"""

import pandas as pd
import numpy as np
from pgmpy.estimators import HillClimbSearch
from pgmpy.models import BayesianNetwork
from pgmpy.estimators import MaximumLikelihoodEstimator
import matplotlib.pyplot as plt
import networkx as nx
import os
import time
import json
from datetime import datetime

# è®¾ç½®ä¸­æ–‡å­—ä½“
import matplotlib
matplotlib.rcParams['font.family'] = ['sans-serif']
matplotlib.rcParams['font.sans-serif'] = [
    'SimHei', 'WenQuanYi Micro Hei', 'WenQuanYi Zen Hei', 
    'Noto Sans CJK SC', 'Source Han Sans SC', 'Microsoft YaHei',
    'DejaVu Sans', 'Arial Unicode MS', 'Liberation Sans'
]
matplotlib.rcParams['axes.unicode_minus'] = False

def _detect_compute_mode():
    try:
        import torch
        if torch.cuda.is_available():
            return "gpu"
    except Exception:
        pass
    return "cpu"

def load_data():
    """åŠ è½½æ•°æ®"""
    input_file = "/home/zkr/å› æžœå‘çŽ°3/01æ•°æ®é¢„å¤„ç†/ç¼©å‡æ•°æ®_è§„æ ¼.csv"
    
    # å°è¯•ä½¿ç”¨utf-8ç¼–ç 
    try:
        df = pd.read_csv(input_file, encoding='utf-8', header=0, index_col=0)
    except UnicodeDecodeError:
        try:
            df = pd.read_csv(input_file, encoding='utf-8-sig', header=0, index_col=0)
        except UnicodeDecodeError:
            df = pd.read_csv(input_file, encoding='latin-1', header=0, index_col=0)
    
    df = df.dropna(axis=1, how='all')
    df = df.astype('float32')
    
    print(f"âœ“ æ•°æ®åŠ è½½å®Œæˆ: {df.shape}")
    return df

class AICDScoreCached:
    def __init__(self, df, mode="cpu"):
        self.mode = mode
        self.df = df.copy()
        self.cols = list(self.df.columns)
        self.n = len(self.cols)
        self.codes = []
        self.card = []
        for c in self.cols:
            cat = pd.Categorical(self.df[c])
            code = cat.codes.astype(np.int64)
            self.codes.append(code)
            self.card.append(int(code.max() + 1))
        self.codes = np.vstack(self.codes).T
        self.card = np.array(self.card, dtype=np.int64)
        self.cache = {}
        self.device = None
        self.use_gpu = False
        if self.mode == "gpu":
            try:
                import torch
                if torch.cuda.is_available():
                    self.device = torch.device("cuda")
                    self.tcodes = torch.tensor(self.codes, dtype=torch.int64, device=self.device)
                    self.tcard = torch.tensor(self.card, dtype=torch.int64, device=self.device)
                    self.use_gpu = True
            except Exception:
                self.use_gpu = False

    def local_score(self, var, parents):
        if isinstance(var, str):
            vi = self.cols.index(var)
        else:
            vi = int(var)
        pidx = []
        for p in parents or []:
            pidx.append(self.cols.index(p) if isinstance(p, str) else int(p))
        key = (vi, tuple(sorted(pidx)))
        if key in self.cache:
            return self.cache[key]
        r = int(self.card[vi])
        if len(pidx) == 0:
            x = self.codes[:, vi]
            if self.use_gpu:
                import torch
                tx = self.tcodes[:, vi]
                counts = torch.bincount(tx, minlength=r).double()
                total = counts.sum()
                probs = (counts + 1e-12) / (total + 1e-12)
                ll = float((counts * torch.log(probs)).sum().item())
            else:
                counts = np.bincount(x, minlength=r).astype(np.float64)
                total = counts.sum()
                probs = (counts + 1e-12) / (total + 1e-12)
                ll = float((counts * np.log(probs)).sum())
            num_parents_states = 1
            score = ll - num_parents_states * (r - 1)
            self.cache[key] = score
            return score
        strides = []
        q = 1
        for idx in pidx:
            strides.append(q)
            q *= int(self.card[idx])
        if self.use_gpu:
            import torch
            gp = torch.zeros(self.tcodes.shape[0], dtype=torch.int64, device=self.device)
            for s, idx in zip(strides, pidx):
                gp = gp + self.tcodes[:, idx] * int(s)
            combined = gp * r + self.tcodes[:, vi]
            counts = torch.bincount(combined, minlength=q * r).double()
            mat = counts.view(q, r)
            n_pa = mat.sum(dim=1)
            probs = (mat + 1e-12) / (n_pa.unsqueeze(1) + 1e-12)
            ll = float((mat * torch.log(probs)).sum().item())
        else:
            gp = np.zeros(self.codes.shape[0], dtype=np.int64)
            for s, idx in zip(strides, pidx):
                gp = gp + self.codes[:, idx] * int(s)
            combined = gp * r + self.codes[:, vi]
            counts = np.bincount(combined, minlength=q * r).astype(np.float64)
            mat = counts.reshape(q, r)
            n_pa = mat.sum(axis=1)
            probs = (mat + 1e-12) / (n_pa[:, None] + 1e-12)
            ll = float((mat * np.log(probs)).sum())
        num_parents_states = q
        score = ll - num_parents_states * (r - 1)
        self.cache[key] = score
        return score

def hill_climb_gpu(df, max_indegree=None, epsilon=1e-4, mode="cpu"):
    cols = list(df.columns)
    G = nx.DiGraph()
    G.add_nodes_from(cols)
    score = AICDScoreCached(df, mode=mode)
    parents = {c: [] for c in cols}
    local = {c: score.local_score(c, []) for c in cols}
    while True:
        best = None
        best_delta = None
        best_op = None
        for u in cols:
            for v in cols:
                if u == v:
                    continue
                if G.has_edge(u, v):
                    continue
                if max_indegree is not None and len(parents[v]) >= max_indegree:
                    continue
                if nx.has_path(G, v, u):
                    continue
                new_parents = parents[v] + [u]
                new_score = score.local_score(v, new_parents)
                delta = new_score - local[v]
                if (best_delta is None) or (delta > best_delta):
                    best_delta = delta
                    best = (u, v)
                    best_op = "add"
        for u, v in list(G.edges()):
            new_parents = [p for p in parents[v] if p != u]
            new_score = score.local_score(v, new_parents)
            delta = new_score - local[v]
            if (best_delta is None) or (delta > best_delta):
                best_delta = delta
                best = (u, v)
                best_op = "remove"
        for u, v in list(G.edges()):
            if max_indegree is not None and len(parents[u]) >= max_indegree:
                continue
            if nx.has_path(G, u, v):
                continue
            if nx.has_path(G, v, u):
                continue
            new_parents_v = [p for p in parents[v] if p != u]
            new_parents_u = parents[u] + [v]
            s_v = score.local_score(v, new_parents_v)
            s_u = score.local_score(u, new_parents_u)
            delta = (s_v - local[v]) + (s_u - local[u])
            if (best_delta is None) or (delta > best_delta):
                best_delta = delta
                best = (u, v)
                best_op = "reverse"
        if best_op is None or (best_delta is None) or (best_delta < epsilon):
            break
        u, v = best
        if best_op == "add":
            G.add_edge(u, v)
            parents[v] = parents[v] + [u]
            local[v] = score.local_score(v, parents[v])
        elif best_op == "remove":
            G.remove_edge(u, v)
            parents[v] = [p for p in parents[v] if p != u]
            local[v] = score.local_score(v, parents[v])
        elif best_op == "reverse":
            G.remove_edge(u, v)
            parents[v] = [p for p in parents[v] if p != u]
            local[v] = score.local_score(v, parents[v])
            G.add_edge(v, u)
            parents[u] = parents[u] + [v]
            local[u] = score.local_score(u, parents[u])
    return G

def create_output_folder():
    """åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹"""
    script_dir = os.path.dirname(os.path.abspath(__file__))
    output_dir = os.path.join(script_dir, "02çˆ¬å±±ç®—æ³•ç»“æžœ")
    os.makedirs(output_dir, exist_ok=True)
    return output_dir

def save_dag_results(dag, output_folder, df_columns):
    """ä¿å­˜DAGç»“æžœåˆ°æ–‡ä»¶"""
    edges = list(dag.edges())
    
    # ä¿å­˜TXTæ ¼å¼
    txt_file = os.path.join(output_folder, "HillClimbing_AIC-D_å› æžœè¾¹å®Œæ•´.txt")
    with open(txt_file, 'w', encoding='utf-8') as f:
        f.write("çˆ¬å±±ç®—æ³• (AIC-D) å‘çŽ°çš„å› æžœè¾¹\n")
        f.write("=" * 40 + "\n")
        for i, edge in enumerate(edges, 1):
            f.write(f"{i:3d}. {edge[0]} -> {edge[1]}\n")
    
    # ä¿å­˜CSVæ ¼å¼
    df_edges = pd.DataFrame(edges, columns=["æºèŠ‚ç‚¹", "ç›®æ ‡èŠ‚ç‚¹"])
    csv_file = os.path.join(output_folder, "HillClimbing_AIC-D_å› æžœè¾¹åˆ—è¡¨.csv")
    df_edges.to_csv(csv_file, index=False, encoding="utf-8-sig")
    
    # ç”Ÿæˆç½‘ç»œå›¾
    plt.figure(figsize=(16, 12))
    G = nx.DiGraph()
    G.add_edges_from(edges)
    
    pos = nx.spring_layout(G, k=3, iterations=50, seed=42)
    
    # ç»˜åˆ¶èŠ‚ç‚¹
    nx.draw_networkx_nodes(G, pos, 
                          node_color='lightcoral', 
                          node_size=2000,
                          alpha=0.8)
    
    # ç»˜åˆ¶è¾¹
    nx.draw_networkx_edges(G, pos, 
                          edge_color='gray',
                          arrows=True,
                          arrowsize=20,
                          arrowstyle='->',
                          width=1.5,
                          alpha=0.7)
    
    # ç»˜åˆ¶æ ‡ç­¾
    nx.draw_networkx_labels(G, pos, 
                           font_size=10,
                           font_weight='bold',
                           font_family='sans-serif')
    
    plt.title(f"çˆ¬å±±ç®—æ³• (AIC-D) å› æžœç½‘ç»œå›¾\nå…±{len(edges)}æ¡å› æžœè¾¹", fontsize=16, fontweight='bold')
    plt.axis('off')
    plt.tight_layout()
    
    graph_file = os.path.join(output_folder, "HillClimbing_AIC-D_å› æžœç½‘ç»œå›¾.png")
    plt.savefig(graph_file, dpi=300, bbox_inches='tight', 
                facecolor='white', edgecolor='none')
    plt.close()
    
    # åˆ›å»ºè¯¦ç»†JSONç»“æžœ
    G = nx.DiGraph()
    G.add_edges_from(edges)
    
    in_degrees = dict(G.in_degree())
    out_degrees = dict(G.out_degree())
    
    results = {
        "ç®—æ³•ä¿¡æ¯": {
            "ç®—æ³•åç§°": "çˆ¬å±±ç®—æ³• (Hill Climbing Search)",
            "è¯„åˆ†æ–¹æ³•": "AIC-D (Akaike Information Criterion - Discrete)",
            "ç”Ÿæˆæ—¶é—´": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "æ•°æ®ç»´åº¦": {
                "æ ·æœ¬æ•°": len(df_columns),
                "å˜é‡æ•°": len(df_columns)
            }
        },
        "ç½‘ç»œç»“æž„": {
            "èŠ‚ç‚¹æ€»æ•°": len(dag.nodes()),
            "è¾¹æ€»æ•°": len(edges),
            "èŠ‚ç‚¹åˆ—è¡¨": list(dag.nodes()),
            "å› æžœè¾¹åˆ—è¡¨": [{"æºèŠ‚ç‚¹": edge[0], "ç›®æ ‡èŠ‚ç‚¹": edge[1]} for edge in edges]
        },
        "ç»Ÿè®¡ä¿¡æ¯": {
            "å…¥åº¦ç»Ÿè®¡": {node: in_degrees.get(node, 0) for node in dag.nodes()},
            "å‡ºåº¦ç»Ÿè®¡": {node: out_degrees.get(node, 0) for node in dag.nodes()},
            "æœ€å¤§å…¥åº¦": max(in_degrees.values()) if in_degrees else 0,
            "æœ€å¤§å‡ºåº¦": max(out_degrees.values()) if out_degrees else 0,
            "å¹³å‡åº¦æ•°": sum(dict(G.degree()).values()) / len(dag.nodes()) if dag.nodes() else 0
        },
        "èŠ‚ç‚¹åˆ†æž": {
            "æ ¹èŠ‚ç‚¹": [node for node in dag.nodes() if in_degrees.get(node, 0) == 0],
            "å¶èŠ‚ç‚¹": [node for node in dag.nodes() if out_degrees.get(node, 0) == 0],
            "ä¸­ä»‹èŠ‚ç‚¹": [node for node in dag.nodes() if in_degrees.get(node, 0) > 0 and out_degrees.get(node, 0) > 0]
        }
    }
    
    json_file = os.path.join(output_folder, "HillClimbing_AIC-D_å› æžœç»“æžœ.json")
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    return txt_file, csv_file, graph_file, json_file, results

def run_hillclimbing_algorithm():
    """è¿è¡Œçˆ¬å±±ç®—æ³•"""
    print("=" * 60)
    print("02 çˆ¬å±±ç®—æ³• (Hill Climbing Search) - å¼€å§‹æ‰§è¡Œ")
    print("=" * 60)
    
    # 1. åŠ è½½æ•°æ®
    df = load_data()
    
    # 2. åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹
    output_dir = create_output_folder()
    
    mode = _detect_compute_mode()
    print(f"è®¡ç®—æ¨¡å¼: {mode}")
    print("æ­£åœ¨è¿è¡Œçˆ¬å±±ç®—æ³• (AIC-Dè¯„åˆ†)...")
    start_time = time.time()
    
    try:
        if mode == "gpu":
            dag_graph = hill_climb_gpu(df, mode="gpu")
            dag = nx.DiGraph(dag_graph)
        else:
            hc = HillClimbSearch(df)
            dag = hc.estimate(scoring_method='aic-d')
        
        end_time = time.time()
        execution_time = end_time - start_time
        
        print(f"âœ“ çˆ¬å±±ç®—æ³•å®Œæˆï¼Œè€—æ—¶: {execution_time:.2f}ç§’")
        print(f"âœ“ å‘çŽ° {len(dag.edges())} æ¡å› æžœè¾¹")
        
        # 4. ä¿å­˜ç»“æžœ
        txt_file, csv_file, graph_file, json_file, results = save_dag_results(dag, output_dir, df.columns)
        
        # 5. è¾“å‡ºç»“æžœæ‘˜è¦
        print("\n" + "=" * 60)
        print("çˆ¬å±±ç®—æ³•æ‰§è¡Œå®Œæˆ - ç»“æžœæ‘˜è¦")
        print("=" * 60)
        print(f"è¯„åˆ†æ–¹æ³•: AIC-D")
        print(f"æ‰§è¡Œæ—¶é—´: {execution_time:.2f}ç§’")
        print(f"æ•°æ®ç»´åº¦: {df.shape[0]} Ã— {df.shape[1]}")
        print(f"å‘çŽ°çš„å› æžœè¾¹æ•°é‡: {results['ç½‘ç»œç»“æž„']['è¾¹æ€»æ•°']}")
        print(f"ç½‘ç»œèŠ‚ç‚¹æ•°é‡: {results['ç½‘ç»œç»“æž„']['èŠ‚ç‚¹æ€»æ•°']}")
        print(f"æ ¹èŠ‚ç‚¹æ•°é‡: {len(results['èŠ‚ç‚¹åˆ†æž']['æ ¹èŠ‚ç‚¹'])}")
        print(f"å¶èŠ‚ç‚¹æ•°é‡: {len(results['èŠ‚ç‚¹åˆ†æž']['å¶èŠ‚ç‚¹'])}")
        print(f"ä¸­ä»‹èŠ‚ç‚¹æ•°é‡: {len(results['èŠ‚ç‚¹åˆ†æž']['ä¸­ä»‹èŠ‚ç‚¹'])}")
        print(f"å¹³å‡èŠ‚ç‚¹åº¦æ•°: {results['ç»Ÿè®¡ä¿¡æ¯']['å¹³å‡åº¦æ•°']:.2f}")
        
        print(f"\nðŸ“ ç»“æžœä¿å­˜ä½ç½®:")
        print(f"  - TXTæ–‡ä»¶: {txt_file}")
        print(f"  - CSVæ–‡ä»¶: {csv_file}")
        print(f"  - ç½‘ç»œå›¾: {graph_file}")
        print(f"  - JSONç»“æžœ: {json_file}")
        
        return output_dir, len(dag.edges())
        
    except Exception as e:
        print(f"âŒ çˆ¬å±±ç®—æ³•æ‰§è¡Œå¤±è´¥: {str(e)}")
        raise

if __name__ == "__main__":
    try:
        output_dir, edge_count = run_hillclimbing_algorithm()
        print(f"\nâœ… 02 çˆ¬å±±ç®—æ³•æ‰§è¡ŒæˆåŠŸï¼å‘çŽ° {edge_count} æ¡å› æžœè¾¹")
    except Exception as e:
        print(f"\nâŒ 02 çˆ¬å±±ç®—æ³•æ‰§è¡Œå¤±è´¥: {str(e)}")
        raise
```

## 03 è´ªå©ªç­‰ä»·æœç´¢ (GES)

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
03 è´ªå©ªç­‰ä»·æœç´¢ (Greedy Equivalence Search - GES)
éžäº¤äº’å¼ç‰ˆæœ¬ï¼Œä½¿ç”¨AIC-Dè¯„åˆ†æ ‡å‡†

ä½œè€…: å› æžœå‘çŽ°ç³»ç»Ÿ
æ—¥æœŸ: 2025å¹´

çŽ¯å¢ƒä¾èµ–:
- å¯é€‰GPUåŠ é€Ÿ: éœ€è¦å®‰è£…`torch>=2.1`ï¼ŒNVIDIAé©±åŠ¨ä¸ŽCUDA(å»ºè®®11.8æˆ–12.x)
- æ— GPUæˆ–æ˜¾å­˜ä¸è¶³æ—¶å°†è‡ªåŠ¨å›žé€€åˆ°CPUå¹¶ç»§ç»­è¿è¡Œ
- å¯é€šè¿‡çŽ¯å¢ƒå˜é‡`GES_FORCE_CPU=1`å¼ºåˆ¶ä½¿ç”¨CPU
"""

import pandas as pd
import numpy as np
from pgmpy.estimators import GES, StructureScore
import matplotlib.pyplot as plt
import networkx as nx
import os
import time
import json
from datetime import datetime

# è®¾ç½®ä¸­æ–‡å­—ä½“
import matplotlib
matplotlib.rcParams['font.family'] = ['sans-serif']
matplotlib.rcParams['font.sans-serif'] = [
    'SimHei', 'WenQuanYi Micro Hei', 'WenQuanYi Zen Hei', 
    'Noto Sans CJK SC', 'Source Han Sans SC', 'Microsoft YaHei',
    'DejaVu Sans', 'Arial Unicode MS', 'Liberation Sans'
]
matplotlib.rcParams['axes.unicode_minus'] = False

def _detect_compute_mode():
    try:
        import torch
        import os as _os
        if _os.environ.get("GES_FORCE_CPU", "0") == "1":
            return "cpu"
        if torch.cuda.is_available():
            return "gpu"
    except Exception:
        pass
    return "cpu"

def load_data():
    """åŠ è½½æ•°æ®"""
    input_file = "/home/zkr/å› æžœå‘çŽ°3/01æ•°æ®é¢„å¤„ç†/ç¼©å‡æ•°æ®_è§„æ ¼.csv"
    
    # å°è¯•ä½¿ç”¨utf-8ç¼–ç 
    try:
        df = pd.read_csv(input_file, encoding='utf-8', header=0, index_col=0)
    except UnicodeDecodeError:
        try:
            df = pd.read_csv(input_file, encoding='utf-8-sig', header=0, index_col=0)
        except UnicodeDecodeError:
            df = pd.read_csv(input_file, encoding='latin-1', header=0, index_col=0)
    
    df = df.dropna(axis=1, how='all')
    df = df.astype('float32')
    
    print(f"âœ“ æ•°æ®åŠ è½½å®Œæˆ: {df.shape}")
    return df

class AICDScoreGPU(StructureScore):
    def __init__(self, data, mode="cpu", **kwargs):
        super(AICDScoreGPU, self).__init__(data=data, **kwargs)
        self.mode = mode
        self.cols = list(self.data.columns)
        self.codes = []
        self.card = []
        for c in self.cols:
            cat = pd.Categorical(self.data[c])
            code = cat.codes.astype(np.int64)
            self.codes.append(code)
            self.card.append(int((code.max() + 1) if code.size else 0))
        self.codes = np.vstack(self.codes).T if len(self.codes) > 0 else np.empty((0, 0), dtype=np.int64)
        self.card = np.array(self.card, dtype=np.int64)
        self.cache = {}
        self.device = None
        self.use_gpu = False
        if self.mode == "gpu":
            try:
                import torch
                if torch.cuda.is_available() and self.codes.size > 0:
                    self.device = torch.device("cuda")
                    try:
                        self.tcodes = torch.tensor(self.codes, dtype=torch.int64, device=self.device)
                        self.tcard = torch.tensor(self.card, dtype=torch.int64, device=self.device)
                        self.use_gpu = True
                    except RuntimeError as e:
                        if "out of memory" in str(e).lower() or "cuda" in str(e).lower():
                            try:
                                torch.cuda.empty_cache()
                            except Exception:
                                pass
                            self.use_gpu = False
                else:
                    self.use_gpu = False
            except Exception:
                self.use_gpu = False

    def _local_score_cpu(self, vi, pidx):
        r = int(self.card[vi])
        if r == 0:
            return 0.0
        if len(pidx) == 0:
            x = self.codes[:, vi]
            counts = np.bincount(x, minlength=r).astype(np.float64)
            total = counts.sum()
            if total == 0:
                return 0.0
            mask = counts > 0
            ll = float((counts[mask] * (np.log(counts[mask]) - np.log(total))).sum())
            num_parents_states = 1
            return ll - num_parents_states * (r - 1)
        strides = []
        q = 1
        for idx in pidx:
            strides.append(q)
            q *= int(self.card[idx])
        if q == 0:
            return 0.0
        gp = np.zeros(self.codes.shape[0], dtype=np.int64)
        for s, idx in zip(strides, pidx):
            gp = gp + self.codes[:, idx] * int(s)
        combined = gp * r + self.codes[:, vi]
        counts = np.bincount(combined, minlength=q * r).astype(np.float64)
        mat = counts.reshape(q, r)
        n_pa = mat.sum(axis=1)
        rows = n_pa > 0
        if np.any(rows):
            mat2 = mat[rows]
            npa2 = n_pa[rows][:, None]
            ll = float((mat2 * (np.log(np.clip(mat2, 1, None)) - np.log(npa2))).sum())
        else:
            ll = 0.0
        num_parents_states = q
        return ll - num_parents_states * (r - 1)

    def _local_score_gpu(self, vi, pidx):
        import torch
        r = int(self.card[vi])
        if r == 0:
            return 0.0
        try:
            if len(pidx) == 0:
                tx = self.tcodes[:, vi]
                counts = torch.bincount(tx, minlength=r).double()
                total = counts.sum()
                if float(total.item()) == 0.0:
                    return 0.0
                mask = counts > 0
                ll = float((counts[mask] * (torch.log(counts[mask]) - torch.log(total))).sum().item())
                num_parents_states = 1
                return ll - num_parents_states * (r - 1)
            strides = []
            q = 1
            for idx in pidx:
                strides.append(q)
                q *= int(self.card[idx])
            if q == 0:
                return 0.0
            gp = torch.zeros(self.tcodes.shape[0], dtype=torch.int64, device=self.device)
            for s, idx in zip(strides, pidx):
                gp = gp + self.tcodes[:, idx] * int(s)
            combined = gp * r + self.tcodes[:, vi]
            counts = torch.bincount(combined, minlength=q * r).double()
            mat = counts.view(q, r)
            n_pa = mat.sum(dim=1)
            rows_mask = n_pa > 0
            if torch.any(rows_mask):
                mat2 = mat[rows_mask]
                npa2 = n_pa[rows_mask].unsqueeze(1)
                ll = float((mat2 * (torch.log(mat2.clamp_min(1)) - torch.log(npa2))).sum().item())
            else:
                ll = 0.0
            num_parents_states = q
            return ll - num_parents_states * (r - 1)
        except RuntimeError as e:
            if "CUDA" in str(e):
                self.use_gpu = False
                return self._local_score_cpu(vi, pidx)
            raise

    def local_score(self, variable, parents):
        vi = self.cols.index(variable) if isinstance(variable, str) else int(variable)
        pidx = [(self.cols.index(p) if isinstance(p, str) else int(p)) for p in (parents or [])]
        key = (vi, tuple(sorted(pidx)))
        cached = self.cache.get(key)
        if cached is not None:
            return cached
        if self.use_gpu:
            val = self._local_score_gpu(vi, pidx)
        else:
            val = self._local_score_cpu(vi, pidx)
        self.cache[key] = val
        return val

    def structure_prior(self, model):
        return 0

    def structure_prior_ratio(self, operation):
        return 0

def create_output_folder():
    """åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹"""
    script_dir = os.path.dirname(os.path.abspath(__file__))
    output_dir = os.path.join(script_dir, "03è´ªå©ªç­‰ä»·æœç´¢ç»“æžœ")
    os.makedirs(output_dir, exist_ok=True)
    return output_dir

def save_dag_results(dag, output_folder, df_columns):
    """ä¿å­˜DAGç»“æžœåˆ°æ–‡ä»¶"""
    edges = list(dag.edges())
    
    # ä¿å­˜TXTæ ¼å¼
    txt_file = os.path.join(output_folder, "GreedyEquivalence_AIC-D_å› æžœè¾¹å®Œæ•´.txt")
    with open(txt_file, 'w', encoding='utf-8') as f:
        f.write("è´ªå©ªç­‰ä»·æœç´¢ (GES-AIC-D) å‘çŽ°çš„å› æžœè¾¹\n")
        f.write("=" * 40 + "\n")
        for i, edge in enumerate(edges, 1):
            f.write(f"{i:3d}. {edge[0]} -> {edge[1]}\n")
    
    # ä¿å­˜CSVæ ¼å¼
    df_edges = pd.DataFrame(edges, columns=["æºèŠ‚ç‚¹", "ç›®æ ‡èŠ‚ç‚¹"])
    csv_file = os.path.join(output_folder, "GreedyEquivalence_AIC-D_å› æžœè¾¹åˆ—è¡¨.csv")
    df_edges.to_csv(csv_file, index=False, encoding="utf-8-sig")
    
    # ç”Ÿæˆç½‘ç»œå›¾
    plt.figure(figsize=(16, 12))
    G = nx.DiGraph()
    G.add_edges_from(edges)
    
    pos = nx.spring_layout(G, k=3, iterations=50, seed=42)
    
    # ç»˜åˆ¶èŠ‚ç‚¹
    nx.draw_networkx_nodes(G, pos, 
                          node_color='lightgreen', 
                          node_size=2000,
                          alpha=0.8)
    
    # ç»˜åˆ¶è¾¹
    nx.draw_networkx_edges(G, pos, 
                          edge_color='gray',
                          arrows=True,
                          arrowsize=20,
                          arrowstyle='->',
                          width=1.5,
                          alpha=0.7)
    
    # ç»˜åˆ¶æ ‡ç­¾
    nx.draw_networkx_labels(G, pos, 
                           font_size=10,
                           font_weight='bold',
                           font_family='sans-serif')
    
    plt.title(f"è´ªå©ªç­‰ä»·æœç´¢ (GES-AIC-D) å› æžœç½‘ç»œå›¾\nå…±{len(edges)}æ¡å› æžœè¾¹", fontsize=16, fontweight='bold')
    plt.axis('off')
    plt.tight_layout()
    
    graph_file = os.path.join(output_folder, "GreedyEquivalence_AIC-D_å› æžœç½‘ç»œå›¾.png")
    plt.savefig(graph_file, dpi=300, bbox_inches='tight', 
                facecolor='white', edgecolor='none')
    plt.close()
    
    # åˆ›å»ºè¯¦ç»†JSONç»“æžœ
    G = nx.DiGraph()
    G.add_edges_from(edges)
    
    in_degrees = dict(G.in_degree())
    out_degrees = dict(G.out_degree())
    
    results = {
        "ç®—æ³•ä¿¡æ¯": {
            "ç®—æ³•åç§°": "è´ªå©ªç­‰ä»·æœç´¢ (Greedy Equivalence Search - GES)",
            "è¯„åˆ†æ–¹æ³•": "AIC-D (Akaike Information Criterion - Discrete)",
            "ç”Ÿæˆæ—¶é—´": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "æ•°æ®ç»´åº¦": {
                "æ ·æœ¬æ•°": len(df_columns),
                "å˜é‡æ•°": len(df_columns)
            }
        },
        "ç½‘ç»œç»“æž„": {
            "èŠ‚ç‚¹æ€»æ•°": len(dag.nodes()),
            "è¾¹æ€»æ•°": len(edges),
            "èŠ‚ç‚¹åˆ—è¡¨": list(dag.nodes()),
            "å› æžœè¾¹åˆ—è¡¨": [{"æºèŠ‚ç‚¹": edge[0], "ç›®æ ‡èŠ‚ç‚¹": edge[1]} for edge in edges]
        },
        "ç»Ÿè®¡ä¿¡æ¯": {
            "å…¥åº¦ç»Ÿè®¡": {node: in_degrees.get(node, 0) for node in dag.nodes()},
            "å‡ºåº¦ç»Ÿè®¡": {node: out_degrees.get(node, 0) for node in dag.nodes()},
            "æœ€å¤§å…¥åº¦": max(in_degrees.values()) if in_degrees else 0,
            "æœ€å¤§å‡ºåº¦": max(out_degrees.values()) if out_degrees else 0,
            "å¹³å‡åº¦æ•°": sum(dict(G.degree()).values()) / len(dag.nodes()) if dag.nodes() else 0
        },
        "èŠ‚ç‚¹åˆ†æž": {
            "æ ¹èŠ‚ç‚¹": [node for node in dag.nodes() if in_degrees.get(node, 0) == 0],
            "å¶èŠ‚ç‚¹": [node for node in dag.nodes() if out_degrees.get(node, 0) == 0],
            "ä¸­ä»‹èŠ‚ç‚¹": [node for node in dag.nodes() if in_degrees.get(node, 0) > 0 and out_degrees.get(node, 0) > 0]
        }
    }
    
    json_file = os.path.join(output_folder, "GreedyEquivalence_AIC-D_å› æžœç»“æžœ.json")
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    return txt_file, csv_file, graph_file, json_file, results

def run_ges_algorithm():
    """è¿è¡Œè´ªå©ªç­‰ä»·æœç´¢ç®—æ³•"""
    print("=" * 60)
    print("03 è´ªå©ªç­‰ä»·æœç´¢ (GES) - å¼€å§‹æ‰§è¡Œ")
    print("=" * 60)
    
    # 1. åŠ è½½æ•°æ®
    df = load_data()
    
    # 2. åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹
    output_dir = create_output_folder()
    
    # 3. ä½¿ç”¨AIC-Dè¯„åˆ†æ–¹æ³•è¿è¡ŒGESç®—æ³•
    mode = _detect_compute_mode()
    print(f"è®¡ç®—æ¨¡å¼: {mode}")
    print("æ­£åœ¨è¿è¡Œè´ªå©ªç­‰ä»·æœç´¢ (GES-AIC-Dè¯„åˆ†)...")
    if mode == "gpu":
        print("GPUåŠ é€Ÿå¯ç”¨: éœ€è¦å®‰è£…`torch`å¹¶ä¸”å¯ç”¨CUDAé©±åŠ¨ (å»ºè®®CUDA 11.8/12.x)ã€‚æ˜¾å­˜ä¸è¶³å°†è‡ªåŠ¨å›žé€€CPUã€‚")
    start_time = time.time()
    
    try:
        ges = GES(df)
        if mode == "gpu":
            score = AICDScoreGPU(df, mode="gpu")
            dag = ges.estimate(scoring_method=score)
        else:
            dag = ges.estimate(scoring_method='aic-d')
        
        end_time = time.time()
        execution_time = end_time - start_time
        
        print(f"âœ“ è´ªå©ªç­‰ä»·æœç´¢å®Œæˆï¼Œè€—æ—¶: {execution_time:.2f}ç§’")
        print(f"âœ“ å‘çŽ° {len(dag.edges())} æ¡å› æžœè¾¹")
        
        # 4. ä¿å­˜ç»“æžœ
        txt_file, csv_file, graph_file, json_file, results = save_dag_results(dag, output_dir, df.columns)
        
        # 5. è¾“å‡ºç»“æžœæ‘˜è¦
        print("\n" + "=" * 60)
        print("è´ªå©ªç­‰ä»·æœç´¢æ‰§è¡Œå®Œæˆ - ç»“æžœæ‘˜è¦")
        print("=" * 60)
        print(f"è¯„åˆ†æ–¹æ³•: AIC-D")
        print(f"æ‰§è¡Œæ—¶é—´: {execution_time:.2f}ç§’")
        print(f"æ•°æ®ç»´åº¦: {df.shape[0]} Ã— {df.shape[1]}")
        print(f"å‘çŽ°çš„å› æžœè¾¹æ•°é‡: {results['ç½‘ç»œç»“æž„']['è¾¹æ€»æ•°']}")
        print(f"ç½‘ç»œèŠ‚ç‚¹æ•°é‡: {results['ç½‘ç»œç»“æž„']['èŠ‚ç‚¹æ€»æ•°']}")
        print(f"æ ¹èŠ‚ç‚¹æ•°é‡: {len(results['èŠ‚ç‚¹åˆ†æž']['æ ¹èŠ‚ç‚¹'])}")
        print(f"å¶èŠ‚ç‚¹æ•°é‡: {len(results['èŠ‚ç‚¹åˆ†æž']['å¶èŠ‚ç‚¹'])}")
        print(f"ä¸­ä»‹èŠ‚ç‚¹æ•°é‡: {len(results['èŠ‚ç‚¹åˆ†æž']['ä¸­ä»‹èŠ‚ç‚¹'])}")
        print(f"å¹³å‡èŠ‚ç‚¹åº¦æ•°: {results['ç»Ÿè®¡ä¿¡æ¯']['å¹³å‡åº¦æ•°']:.2f}")
        
        print(f"\nðŸ“ ç»“æžœä¿å­˜ä½ç½®:")
        print(f"  - TXTæ–‡ä»¶: {txt_file}")
        print(f"  - CSVæ–‡ä»¶: {csv_file}")
        print(f"  - ç½‘ç»œå›¾: {graph_file}")
        print(f"  - JSONç»“æžœ: {json_file}")
        
        return output_dir, len(dag.edges())
        
    except Exception as e:
        print(f"âŒ è´ªå©ªç­‰ä»·æœç´¢æ‰§è¡Œå¤±è´¥: {str(e)}")
        raise

if __name__ == "__main__":
    try:
        output_dir, edge_count = run_ges_algorithm()
        print(f"\nâœ… 03 è´ªå©ªç­‰ä»·æœç´¢æ‰§è¡ŒæˆåŠŸï¼å‘çŽ° {edge_count} æ¡å› æžœè¾¹")
    except Exception as e:
        print(f"\nâŒ 03 è´ªå©ªç­‰ä»·æœç´¢æ‰§è¡Œå¤±è´¥: {str(e)}")
        raise
```

## 04 æ ‘æœç´¢ (TAN)

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
04 æ ‘æœç´¢ (Tree Search)
éžäº¤äº’å¼ç‰ˆæœ¬ï¼Œä½¿ç”¨TANæ–¹æ³•ï¼Œé»˜è®¤ç¬¬ä¸€ä¸ªèŠ‚ç‚¹ä½œä¸ºæ ¹èŠ‚ç‚¹

ä½œè€…: å› æžœå‘çŽ°ç³»ç»Ÿ
æ—¥æœŸ: 2025å¹´
"""

import numpy as np
import pandas as pd
import networkx as nx
import matplotlib.pyplot as plt
from pgmpy.estimators import TreeSearch
import os
import time
import json
from datetime import datetime

# è®¾ç½®ä¸­æ–‡å­—ä½“
import matplotlib
matplotlib.rcParams['font.family'] = ['sans-serif']
matplotlib.rcParams['font.sans-serif'] = [
    'SimHei', 'WenQuanYi Micro Hei', 'WenQuanYi Zen Hei', 
    'Noto Sans CJK SC', 'Source Han Sans SC', 'Microsoft YaHei',
    'DejaVu Sans', 'Arial Unicode MS', 'Liberation Sans'
]
matplotlib.rcParams['axes.unicode_minus'] = False

def load_data():
    """åŠ è½½æ•°æ®"""
    input_file = "/home/zkr/å› æžœå‘çŽ°3/01æ•°æ®é¢„å¤„ç†/ç¼©å‡æ•°æ®_è§„æ ¼.csv"
    
    # å°è¯•ä½¿ç”¨utf-8ç¼–ç 
    try:
        df = pd.read_csv(input_file, encoding='utf-8', header=0, index_col=0)
    except UnicodeDecodeError:
        try:
            df = pd.read_csv(input_file, encoding='utf-8-sig', header=0, index_col=0)
        except UnicodeDecodeError:
            df = pd.read_csv(input_file, encoding='latin-1', header=0, index_col=0)
    
    df = df.dropna(axis=1, how='all')
    df = df.astype('float32')
    
    print(f"âœ“ æ•°æ®åŠ è½½å®Œæˆ: {df.shape}")
    return df

def create_output_folder():
    """åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹"""
    script_dir = os.path.dirname(os.path.abspath(__file__))
    output_dir = os.path.join(script_dir, "04æ ‘æœç´¢ç»“æžœ")
    os.makedirs(output_dir, exist_ok=True)
    return output_dir

def save_tree_results(model, output_folder, df_columns, root_node):
    """ä¿å­˜æ ‘æœç´¢ç»“æžœåˆ°æ–‡ä»¶"""
    edges = list(model.edges())
    
    # ä¿å­˜TXTæ ¼å¼
    txt_file = os.path.join(output_folder, "TAN_å› æžœè¾¹å®Œæ•´.txt")
    with open(txt_file, 'w', encoding='utf-8') as f:
        f.write("æ ‘æœç´¢ (TAN) å‘çŽ°çš„å› æžœè¾¹\n")
        f.write("=" * 40 + "\n")
        f.write(f"æ ¹èŠ‚ç‚¹: {root_node}\n")
        f.write("=" * 40 + "\n")
        for i, edge in enumerate(edges, 1):
            f.write(f"{i:3d}. {edge[0]} -> {edge[1]}\n")
    
    # ä¿å­˜CSVæ ¼å¼
    df_edges = pd.DataFrame(edges, columns=["æºèŠ‚ç‚¹", "ç›®æ ‡èŠ‚ç‚¹"])
    csv_file = os.path.join(output_folder, "TAN_å› æžœè¾¹åˆ—è¡¨.csv")
    df_edges.to_csv(csv_file, index=False, encoding="utf-8-sig")
    
    # ç”Ÿæˆç½‘ç»œå›¾
    plt.figure(figsize=(16, 12))
    G = nx.DiGraph()
    G.add_edges_from(edges)
    
    pos = nx.spring_layout(G, k=3, iterations=50, seed=42)
    
    # ç»˜åˆ¶èŠ‚ç‚¹ï¼Œæ ¹èŠ‚ç‚¹ç”¨ä¸åŒé¢œè‰²
    node_colors = ['red' if node == root_node else 'lightyellow' for node in G.nodes()]
    nx.draw_networkx_nodes(G, pos, 
                          node_color=node_colors, 
                          node_size=2000,
                          alpha=0.8)
    
    # ç»˜åˆ¶è¾¹
    nx.draw_networkx_edges(G, pos, 
                          edge_color='gray',
                          arrows=True,
                          arrowsize=20,
                          arrowstyle='->',
                          width=1.5,
                          alpha=0.7)
    
    # ç»˜åˆ¶æ ‡ç­¾
    nx.draw_networkx_labels(G, pos, 
                           font_size=10,
                           font_weight='bold',
                           font_family='sans-serif')
    
    plt.title(f"æ ‘æœç´¢ (TAN) å› æžœç½‘ç»œå›¾\næ ¹èŠ‚ç‚¹: {root_node}\nå…±{len(edges)}æ¡å› æžœè¾¹", 
              fontsize=16, fontweight='bold')
    plt.axis('off')
    plt.tight_layout()
    
    graph_file = os.path.join(output_folder, "TAN_å› æžœç½‘ç»œå›¾.png")
    plt.savefig(graph_file, dpi=300, bbox_inches='tight', 
                facecolor='white', edgecolor='none')
    plt.close()
    
    # åˆ›å»ºè¯¦ç»†JSONç»“æžœ
    G = nx.DiGraph()
    G.add_edges_from(edges)
    
    in_degrees = dict(G.in_degree())
    out_degrees = dict(G.out_degree())
    
    results = {
        "ç®—æ³•ä¿¡æ¯": {
            "ç®—æ³•åç§°": "æ ‘æœç´¢ (Tree Search - TAN)",
            "ä¼°è®¡å™¨ç±»åž‹": "TAN (Tree Augmented Naive Bayes)",
            "æ ¹èŠ‚ç‚¹": root_node,
            "ç”Ÿæˆæ—¶é—´": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "æ•°æ®ç»´åº¦": {
                "æ ·æœ¬æ•°": len(df_columns),
                "å˜é‡æ•°": len(df_columns)
            }
        },
        "ç½‘ç»œç»“æž„": {
            "èŠ‚ç‚¹æ€»æ•°": len(model.nodes()),
            "è¾¹æ€»æ•°": len(edges),
            "èŠ‚ç‚¹åˆ—è¡¨": list(model.nodes()),
            "å› æžœè¾¹åˆ—è¡¨": [{"æºèŠ‚ç‚¹": edge[0], "ç›®æ ‡èŠ‚ç‚¹": edge[1]} for edge in edges]
        },
        "ç»Ÿè®¡ä¿¡æ¯": {
            "å…¥åº¦ç»Ÿè®¡": {node: in_degrees.get(node, 0) for node in model.nodes()},
            "å‡ºåº¦ç»Ÿè®¡": {node: out_degrees.get(node, 0) for node in model.nodes()},
            "æœ€å¤§å…¥åº¦": max(in_degrees.values()) if in_degrees else 0,
            "æœ€å¤§å‡ºåº¦": max(out_degrees.values()) if out_degrees else 0,
            "å¹³å‡åº¦æ•°": sum(dict(G.degree()).values()) / len(model.nodes()) if model.nodes() else 0
        },
        "èŠ‚ç‚¹åˆ†æž": {
            "æ ¹èŠ‚ç‚¹": [node for node in model.nodes() if in_degrees.get(node, 0) == 0],
            "å¶èŠ‚ç‚¹": [node for node in model.nodes() if out_degrees.get(node, 0) == 0],
            "ä¸­ä»‹èŠ‚ç‚¹": [node for node in model.nodes() if in_degrees.get(node, 0) > 0 and out_degrees.get(node, 0) > 0]
        }
    }
    
    json_file = os.path.join(output_folder, "TAN_å› æžœç»“æžœ.json")
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    return txt_file, csv_file, graph_file, json_file, results

def run_tree_search_algorithm():
    """è¿è¡Œæ ‘æœç´¢ç®—æ³•"""
    print("=" * 60)
    print("04 æ ‘æœç´¢ (Tree Search - TAN) - å¼€å§‹æ‰§è¡Œ")
    print("=" * 60)
    
    # 1. åŠ è½½æ•°æ®
    df = load_data()
    
    # 2. åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹
    output_dir = create_output_folder()
    
    # 3. ä½¿ç”¨ç¬¬ä¸€ä¸ªèŠ‚ç‚¹ä½œä¸ºæ ¹èŠ‚ç‚¹
    root_node = df.columns[0]
    print(f"ä½¿ç”¨æ ¹èŠ‚ç‚¹: {root_node}")
    
    # 4. è¿è¡ŒTANç®—æ³•
    print("æ­£åœ¨è¿è¡Œæ ‘æœç´¢ (TANç®—æ³•)...")
    start_time = time.time()
    
    try:
        ts = TreeSearch(df)
        model = ts.estimate(estimator_type='tan', class_node=root_node)
        
        end_time = time.time()
        execution_time = end_time - start_time
        
        print(f"âœ“ æ ‘æœç´¢å®Œæˆï¼Œè€—æ—¶: {execution_time:.2f}ç§’")
        print(f"âœ“ å‘çŽ° {len(model.edges())} æ¡å› æžœè¾¹")
        
        # 5. ä¿å­˜ç»“æžœ
        txt_file, csv_file, graph_file, json_file, results = save_tree_results(model, output_dir, df.columns, root_node)
        
        # 6. è¾“å‡ºç»“æžœæ‘˜è¦
        print("\n" + "=" * 60)
        print("æ ‘æœç´¢æ‰§è¡Œå®Œæˆ - ç»“æžœæ‘˜è¦")
        print("=" * 60)
        print(f"ç®—æ³•ç±»åž‹: TAN (Tree Augmented Naive Bayes)")
        print(f"æ ¹èŠ‚ç‚¹: {root_node}")
        print(f"æ‰§è¡Œæ—¶é—´: {execution_time:.2f}ç§’")
        print(f"æ•°æ®ç»´åº¦: {df.shape[0]} Ã— {df.shape[1]}")
        print(f"å‘çŽ°çš„å› æžœè¾¹æ•°é‡: {results['ç½‘ç»œç»“æž„']['è¾¹æ€»æ•°']}")
        print(f"ç½‘ç»œèŠ‚ç‚¹æ•°é‡: {results['ç½‘ç»œç»“æž„']['èŠ‚ç‚¹æ€»æ•°']}")
        print(f"æ ¹èŠ‚ç‚¹æ•°é‡: {len(results['èŠ‚ç‚¹åˆ†æž']['æ ¹èŠ‚ç‚¹'])}")
        print(f"å¶èŠ‚ç‚¹æ•°é‡: {len(results['èŠ‚ç‚¹åˆ†æž']['å¶èŠ‚ç‚¹'])}")
        print(f"ä¸­ä»‹èŠ‚ç‚¹æ•°é‡: {len(results['èŠ‚ç‚¹åˆ†æž']['ä¸­ä»‹èŠ‚ç‚¹'])}")
        print(f"å¹³å‡èŠ‚ç‚¹åº¦æ•°: {results['ç»Ÿè®¡ä¿¡æ¯']['å¹³å‡åº¦æ•°']:.2f}")
        
        print(f"\nðŸ“ ç»“æžœä¿å­˜ä½ç½®:")
        print(f"  - TXTæ–‡ä»¶: {txt_file}")
        print(f"  - CSVæ–‡ä»¶: {csv_file}")
        print(f"  - ç½‘ç»œå›¾: {graph_file}")
        print(f"  - JSONç»“æžœ: {json_file}")
        
        return output_dir, len(model.edges())
        
    except Exception as e:
        print(f"âŒ æ ‘æœç´¢æ‰§è¡Œå¤±è´¥: {str(e)}")
        raise

if __name__ == "__main__":
    try:
        output_dir, edge_count = run_tree_search_algorithm()
        print(f"\nâœ… 04 æ ‘æœç´¢æ‰§è¡ŒæˆåŠŸï¼å‘çŽ° {edge_count} æ¡å› æžœè¾¹")
    except Exception as e:
        print(f"\nâŒ 04 æ ‘æœç´¢æ‰§è¡Œå¤±è´¥: {str(e)}")
        raise
```

## 05 ä¸“å®¶åœ¨å¾ªçŽ¯ (Expert In The Loop)

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
05 ä¸“å®¶åœ¨å¾ªçŽ¯ (Expert In The Loop)
ä½¿ç”¨LLMè¿›è¡Œæ™ºèƒ½å› æžœæŽ¨æ–­çš„å®Œæ•´ç‰ˆæœ¬

ä½œè€…: å› æžœå‘çŽ°ç³»ç»Ÿ
æ—¥æœŸ: 2025å¹´
"""

from pgmpy.utils import get_example_model, llm_pairwise_orient
from pgmpy.estimators import ExpertInLoop, ExpertKnowledge
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import networkx as nx
import os
import json
from datetime import datetime
import warnings
from sklearn.exceptions import ConvergenceWarning
from sklearn.feature_selection import VarianceThreshold
from sklearn.preprocessing import StandardScaler
import re
from litellm import completion
import pgmpy.utils as pg_utils
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed

# è¿‡æ»¤è­¦å‘Š
warnings.filterwarnings('ignore', category=UserWarning, module='sklearn')
warnings.filterwarnings('ignore', category=RuntimeWarning, module='numpy')
warnings.filterwarnings('ignore', category=ConvergenceWarning)

os.environ.setdefault("OPENAI_BASE_URL", "http://localhost:11434/v1")
os.environ.setdefault("OPENAI_API_KEY", "EMPTY")
LLM_MODEL = os.environ.get("LLM_MODEL", "ollama/qwen2.5:7b")
MODEL_CHOICES = [
    "ollama/qwen2.5:7b",
    "ollama/qwen2.5:32b",
    "ollama/mannix/qwen2-57b:latest",
    "ollama/huihui_ai/deepseek-r1-abliterated:70b",
    "ollama/deepseek-r1:32b",
]
EIL_PVAL_THRESHOLD = float(os.environ.get("EIL_PVAL_THRESHOLD", "0.1"))
EIL_EFFECT_SIZE = float(os.environ.get("EIL_EFFECT_SIZE", "0.2"))
ORIENT_CACHE = {}
LLM_WORKERS = int(os.environ.get("LLM_WORKERS", "10"))
SELECT_PAIRS_THRESHOLD = float(os.environ.get("SELECT_PAIRS_THRESHOLD", "0.45"))
MAX_LLM_PAIRS = int(os.environ.get("MAX_LLM_PAIRS", "120"))
CANDIDATE_SET = set()
LLM_BATCH_SIZE = int(os.environ.get("LLM_BATCH_SIZE", "30"))
FAST_MODE = os.environ.get("FAST_MODE", "1") == "1"
LLM_BATCH_WORKERS = int(os.environ.get("LLM_BATCH_WORKERS", "10"))
SELECTED_PAIRS = []
EDGE_SOURCE = {}
LLM_ONLY = os.environ.get("LLM_ONLY", "1") == "1"

# è®¾ç½®ä¸­æ–‡å­—ä½“
import matplotlib
matplotlib.rcParams['font.family'] = ['sans-serif']
matplotlib.rcParams['font.sans-serif'] = [
    'SimHei', 'WenQuanYi Micro Hei', 'WenQuanYi Zen Hei', 
    'Noto Sans CJK SC', 'Source Han Sans SC', 'Microsoft YaHei',
    'DejaVu Sans', 'Arial Unicode MS', 'Liberation Sans'
]
matplotlib.rcParams['axes.unicode_minus'] = False

def load_data():
    """åŠ è½½æ•°æ®"""
    input_file = "/home/zkr/å› æžœå‘çŽ°3/01æ•°æ®é¢„å¤„ç†/ç¼©å‡æ•°æ®_è§„æ ¼.csv"
    
    # å°è¯•ä½¿ç”¨utf-8ç¼–ç 
    try:
        df = pd.read_csv(input_file, encoding='utf-8', header=0, index_col=0)
    except UnicodeDecodeError:
        try:
            df = pd.read_csv(input_file, encoding='utf-8-sig', header=0, index_col=0)
        except UnicodeDecodeError:
            df = pd.read_csv(input_file, encoding='latin-1', header=0, index_col=0)
    
    df = df.dropna(axis=1, how='all')
    df = df.astype('float32')
    
    print(f"âœ“ æ•°æ®åŠ è½½å®Œæˆ: {df.shape}")
    return df

def create_output_folder():
    """åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹"""
    script_dir = os.path.dirname(os.path.abspath(__file__))
    output_dir = os.path.join(script_dir, "05ä¸“å®¶åœ¨å¾ªçŽ¯ç»“æžœ")
    os.makedirs(output_dir, exist_ok=True)
    return output_dir

def preprocess_data(df):
    """æ•°æ®é¢„å¤„ç†"""
    print("æ­£åœ¨è¿›è¡Œæ•°æ®è´¨é‡æ£€æŸ¥...")
    
    # å¤„ç†NaNå€¼
    if df.isnull().values.any():
        print("æ•°æ®ä¸­å­˜åœ¨ NaN å€¼ï¼Œä½¿ç”¨å‡å€¼å¡«å……")
        df = df.fillna(df.mean())
    
    # ç§»é™¤é›¶æ–¹å·®åˆ—
    zero_var_cols = df.columns[df.var() == 0]
    if not zero_var_cols.empty:
        print(f"ç§»é™¤é›¶æ–¹å·®åˆ—: {list(zero_var_cols)}")
        df = df.drop(columns=zero_var_cols)
    
    # å¤„ç†å¤šé‡å…±çº¿æ€§
    corr_matrix = df.corr().abs()
    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
    to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]
    
    if to_drop:
        df = df.drop(columns=to_drop)
        print(f"ç§»é™¤é«˜åº¦å…±çº¿åˆ—: {to_drop}")
    
    # æ–¹å·®é˜ˆå€¼è¿‡æ»¤
    selector = VarianceThreshold(threshold=0.01)
    df_transformed = selector.fit_transform(df)
    
    if df_transformed.shape[1] < df.shape[1]:
        retained_cols = df.columns[selector.get_support()]
        df = pd.DataFrame(df_transformed, columns=retained_cols, index=df.index)
        print(f"VarianceThresholdç§»é™¤äº† {df.shape[1] - df_transformed.shape[1]} ä¸ªä½Žæ–¹å·®åˆ—")
    
    print(f"âœ“ æ•°æ®é¢„å¤„ç†å®Œæˆï¼Œæœ€ç»ˆç»´åº¦: {df.shape}")
    return df

def create_variable_descriptions(df):
    """åˆ›å»ºå˜é‡æè¿°å­—å…¸"""
    variable_descriptions = {}
    for col in df.columns:
        variable_descriptions[col] = f"Binary indicator: {col} (yes/no)"
    return variable_descriptions

def robust_llm_orient(u, v, variable_descriptions=None, llm_model=LLM_MODEL, **kwargs):
    """ç¨³å¥çš„LLMå®šå‘å‡½æ•°"""
    if variable_descriptions is None:
        variable_descriptions = {}
    
    try:
        # ä½¿ç”¨åŽŸå§‹çš„LLMå®šå‘å‡½æ•°
        result = llm_pairwise_orient(u, v, variable_descriptions, llm_model)
        return result
    except Exception as e:
        print(f"LLMå®šå‘å¤±è´¥ ({u} <-> {v}): {e}")
        # ä½¿ç”¨å­—å…¸åºä½œä¸ºå›žé€€
        return (u, v) if str(u) < str(v) else (v, u)

def _category(x):
    if isinstance(x, str):
        if x.startswith("ç–¾ç—…_"):
            return "ç–¾ç—…"
        if x.startswith("è¯ç‰©_"):
            return "è¯ç‰©"
        if x.startswith("æ£€éªŒ_"):
            return "æ£€éªŒ"
    return "å…¶ä»–"

def rule_based_orient(u, v):
    cu, cv = _category(u), _category(v)
    if cu == "è¯ç‰©" and cv == "æ£€éªŒ":
        return (u, v)
    if cu == "æ£€éªŒ" and cv == "è¯ç‰©":
        return (v, u)
    if cu == "ç–¾ç—…" and cv == "æ£€éªŒ":
        return (u, v)
    if cu == "æ£€éªŒ" and cv == "ç–¾ç—…":
        return (v, u)
    if cu == "ç–¾ç—…" and cv == "è¯ç‰©":
        return (u, v)
    if cu == "è¯ç‰©" and cv == "ç–¾ç—…":
        return (v, u)
    return None

def fast_llm_pairwise_orient(u, v, variable_descriptions=None, llm_model=LLM_MODEL):
    key = (str(u), str(v)) if str(u) < str(v) else (str(v), str(u))
    if key in ORIENT_CACHE:
        return ORIENT_CACHE[key]
    if key not in CANDIDATE_SET:
        rb = rule_based_orient(u, v)
        if rb is not None:
            ORIENT_CACHE[key] = rb
            EDGE_SOURCE[key] = "rule"
            return ORIENT_CACHE[key]
        ORIENT_CACHE[key] = (u, v) if str(u) < str(v) else (v, u)
        EDGE_SOURCE[key] = "fallback"
        return ORIENT_CACHE[key]
    sys_msg = {
        "role": "system",
        "content": "åªè¾“å‡ºä¸€è¡Œï¼Œæ ¼å¼ä¸¥æ ¼ä¸º 'A->B' æˆ– 'B->A'ï¼Œä¸æ·»åŠ ä»»ä½•è§£é‡Šã€‚"
    }
    desc_u = variable_descriptions.get(u, str(u)) if variable_descriptions else str(u)
    desc_v = variable_descriptions.get(v, str(v)) if variable_descriptions else str(v)
    user_msg = {
        "role": "user",
        "content": f"å˜é‡A: {u}ï¼›æè¿°: {desc_u}\nå˜é‡B: {v}ï¼›æè¿°: {desc_v}\nä»…è¾“å‡ºä¸€ä¸ªç»“æžœ: '{u}->{v}' æˆ– '{v}->{u}'ã€‚"
    }
    try:
        resp = completion(model=llm_model, messages=[sys_msg, user_msg], temperature=0, max_tokens=16, timeout=15)
        text = resp["choices"][0]["message"]["content"].strip()
        p1 = rf"{re.escape(str(u))}\s*(?:->|=>|â†’)\s*{re.escape(str(v))}"
        p2 = rf"{re.escape(str(v))}\s*(?:->|=>|â†’)\s*{re.escape(str(u))}"
        if re.search(p1, text, re.IGNORECASE):
            ORIENT_CACHE[key] = (u, v)
            EDGE_SOURCE[key] = "llm"
            return ORIENT_CACHE[key]
        if re.search(p2, text, re.IGNORECASE):
            ORIENT_CACHE[key] = (v, u)
            EDGE_SOURCE[key] = "llm"
            return ORIENT_CACHE[key]
        rb = rule_based_orient(u, v)
        if rb is not None:
            ORIENT_CACHE[key] = rb
            EDGE_SOURCE[key] = "rule"
            return ORIENT_CACHE[key]
        ORIENT_CACHE[key] = (u, v) if str(u) < str(v) else (v, u)
        EDGE_SOURCE[key] = "fallback"
        return ORIENT_CACHE[key]
    except Exception:
        rb = rule_based_orient(u, v)
        if rb is not None:
            ORIENT_CACHE[key] = rb
            EDGE_SOURCE[key] = "rule"
            return ORIENT_CACHE[key]
        ORIENT_CACHE[key] = (u, v) if str(u) < str(v) else (v, u)
        EDGE_SOURCE[key] = "fallback"
        return ORIENT_CACHE[key]

def batched_llm_orient(pairs, variable_descriptions=None, llm_model=LLM_MODEL):
    lines = []
    for u, v in pairs:
        du = variable_descriptions.get(u, str(u)) if variable_descriptions else str(u)
        dv = variable_descriptions.get(v, str(v)) if variable_descriptions else str(v)
        lines.append(f"A={u};desc={du} | B={v};desc={dv}")
    sys_msg = {"role": "system", "content": "ä»…è¾“å‡ºè‹¥å¹²è¡Œï¼Œæ¯è¡Œæ ¼å¼ 'X->Y' æˆ– 'Y->X'ï¼Œä¸å«å…¶å®ƒå†…å®¹ã€‚"}
    user_msg = {"role": "user", "content": "\n".join(lines)}
    try:
        resp = completion(model=llm_model, messages=[sys_msg, user_msg], temperature=0, max_tokens=LLM_BATCH_SIZE*16, timeout=45)
        text = resp["choices"][0]["message"]["content"].strip()
        outs = [t.strip() for t in text.splitlines() if t.strip()]
        assigned = set()
        for out in outs:
            for u, v in pairs:
                k = (str(u), str(v)) if str(u) < str(v) else (str(v), str(u))
                if k in assigned:
                    continue
                p1 = rf"{re.escape(str(u))}\s*(?:->|=>|â†’)\s*{re.escape(str(v))}"
                p2 = rf"{re.escape(str(v))}\s*(?:->|=>|â†’)\s*{re.escape(str(u))}"
                if re.search(p1, out, re.IGNORECASE):
                    ORIENT_CACHE[k] = (u, v)
                    EDGE_SOURCE[k] = "llm"
                    assigned.add(k)
                elif re.search(p2, out, re.IGNORECASE):
                    ORIENT_CACHE[k] = (v, u)
                    EDGE_SOURCE[k] = "llm"
                    assigned.add(k)
        for u, v in pairs:
            k = (str(u), str(v)) if str(u) < str(v) else (str(v), str(u))
            if k in assigned:
                continue
            rb = rule_based_orient(u, v)
            if rb is not None:
                ORIENT_CACHE[k] = rb
                EDGE_SOURCE[k] = "rule"
            else:
                ORIENT_CACHE[k] = (u, v) if str(u) < str(v) else (v, u)
                EDGE_SOURCE[k] = "fallback"
    except Exception:
        for u, v in pairs:
            k = (str(u), str(v)) if str(u) < str(v) else (str(v), str(u))
            rb = rule_based_orient(u, v)
            if rb is not None:
                ORIENT_CACHE[k] = rb
                EDGE_SOURCE[k] = "rule"
            else:
                ORIENT_CACHE[k] = (u, v) if str(u) < str(v) else (v, u)
                EDGE_SOURCE[k] = "fallback"

def precompute_orientations(df, variable_descriptions):
    corr = df.corr().abs()
    pairs = []
    cols = list(df.columns)
    for i in range(len(cols)):
        for j in range(i+1, len(cols)):
            s = corr.loc[cols[i], cols[j]]
            if s >= SELECT_PAIRS_THRESHOLD:
                pairs.append((cols[i], cols[j], float(s)))
    pairs.sort(key=lambda x: x[2], reverse=True)
    global SELECTED_PAIRS
    SELECTED_PAIRS = [(u, v) for u, v, _ in pairs[:MAX_LLM_PAIRS]]
    for u, v in SELECTED_PAIRS:
        k = (str(u), str(v)) if str(u) < str(v) else (str(v), str(u))
        CANDIDATE_SET.add(k)
    raw_count = len(pairs)
    if SELECTED_PAIRS:
        chunks = [SELECTED_PAIRS[i:i+LLM_BATCH_SIZE] for i in range(0, len(SELECTED_PAIRS), LLM_BATCH_SIZE)]
        total = len(chunks)
        print(f"ç›¸å…³é˜ˆå€¼{SELECT_PAIRS_THRESHOLD}ä¸‹å…±{raw_count}å¯¹ï¼›è¿›å…¥LLMå®šå‘{len(SELECTED_PAIRS)}å¯¹ï¼ˆä¸Šé™{MAX_LLM_PAIRS}ï¼‰")
        print(f"é¢„å®šå‘æ‰¹æ¬¡: {total}ï¼Œå€™é€‰å¯¹æ•°: {len(SELECTED_PAIRS)}")
        done = 0
        with ThreadPoolExecutor(max_workers=LLM_BATCH_WORKERS) as ex:
            futs = [ex.submit(batched_llm_orient, ch, variable_descriptions, LLM_MODEL) for ch in chunks]
            for _ in as_completed(futs):
                done += 1
                if done % 2 == 0 or done == total:
                    print(f"é¢„å®šå‘è¿›åº¦: {done}/{total}")

def save_dag_results(dag, output_folder, df_columns):
    """ä¿å­˜DAGç»“æžœåˆ°æ–‡ä»¶"""
    edges = list(dag.edges())
    
    # ä¿å­˜TXTæ ¼å¼
    txt_file = os.path.join(output_folder, "ExpertInLoop_å› æžœè¾¹å®Œæ•´.txt")
    with open(txt_file, 'w', encoding='utf-8') as f:
        f.write("ä¸“å®¶åœ¨å¾ªçŽ¯ (Expert In The Loop) å‘çŽ°çš„å› æžœè¾¹\n")
        f.write("=" * 50 + "\n")
        for i, edge in enumerate(edges, 1):
            f.write(f"{i:3d}. {edge[0]} -> {edge[1]}\n")
    
    # ä¿å­˜CSVæ ¼å¼
    df_edges = pd.DataFrame(edges, columns=["æºèŠ‚ç‚¹", "ç›®æ ‡èŠ‚ç‚¹"])
    csv_file = os.path.join(output_folder, "ExpertInLoop_å› æžœè¾¹åˆ—è¡¨.csv")
    df_edges.to_csv(csv_file, index=False, encoding="utf-8-sig")
    
    # ç”Ÿæˆç½‘ç»œå›¾
    plt.figure(figsize=(16, 12))
    G = nx.DiGraph()
    G.add_edges_from(edges)
    
    if len(edges) > 0:
        pos = nx.spring_layout(G, k=3, iterations=50, seed=42)
        
        # ç»˜åˆ¶èŠ‚ç‚¹
        nx.draw_networkx_nodes(G, pos, 
                              node_color='lightpink', 
                              node_size=2000,
                              alpha=0.8)
        
        # ç»˜åˆ¶è¾¹
        nx.draw_networkx_edges(G, pos, 
                              edge_color='gray',
                              arrows=True,
                              arrowsize=20,
                              arrowstyle='->',
                              width=1.5,
                              alpha=0.7)
        
        # ç»˜åˆ¶æ ‡ç­¾
        nx.draw_networkx_labels(G, pos, 
                               font_size=10,
                               font_weight='bold',
                               font_family='sans-serif')
    
    plt.title(f"ä¸“å®¶åœ¨å¾ªçŽ¯ (Expert In The Loop) å› æžœç½‘ç»œå›¾\nå…±{len(edges)}æ¡å› æžœè¾¹", 
              fontsize=16, fontweight='bold')
    plt.axis('off')
    plt.tight_layout()
    
    graph_file = os.path.join(output_folder, "ExpertInLoop_å› æžœç½‘ç»œå›¾.png")
    plt.savefig(graph_file, dpi=300, bbox_inches='tight', 
                facecolor='white', edgecolor='none')
    plt.close()
    
    # åˆ›å»ºè¯¦ç»†JSONç»“æžœ
    G = nx.DiGraph()
    G.add_edges_from(edges)
    
    in_degrees = dict(G.in_degree())
    out_degrees = dict(G.out_degree())
    
    results = {
        "ç®—æ³•ä¿¡æ¯": {
            "ç®—æ³•åç§°": "ä¸“å®¶åœ¨å¾ªçŽ¯ (Expert In The Loop)",
            "ç­–ç•¥": "LLMæ™ºèƒ½å®šå‘",
            "ç”Ÿæˆæ—¶é—´": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "æ•°æ®ç»´åº¦": {
                "æ ·æœ¬æ•°": len(df_columns),
                "å˜é‡æ•°": len(df_columns)
            }
        },
        "ç½‘ç»œç»“æž„": {
            "èŠ‚ç‚¹æ€»æ•°": len(dag.nodes()),
            "è¾¹æ€»æ•°": len(edges),
            "èŠ‚ç‚¹åˆ—è¡¨": list(dag.nodes()),
            "å› æžœè¾¹åˆ—è¡¨": [{"æºèŠ‚ç‚¹": edge[0], "ç›®æ ‡èŠ‚ç‚¹": edge[1]} for edge in edges]
        },
        "ç»Ÿè®¡ä¿¡æ¯": {
            "å…¥åº¦ç»Ÿè®¡": {node: in_degrees.get(node, 0) for node in dag.nodes()},
            "å‡ºåº¦ç»Ÿè®¡": {node: out_degrees.get(node, 0) for node in dag.nodes()},
            "æœ€å¤§å…¥åº¦": max(in_degrees.values()) if in_degrees else 0,
            "æœ€å¤§å‡ºåº¦": max(out_degrees.values()) if out_degrees else 0,
            "å¹³å‡åº¦æ•°": sum(dict(G.degree()).values()) / len(dag.nodes()) if dag.nodes() else 0
        },
        "èŠ‚ç‚¹åˆ†æž": {
            "æ ¹èŠ‚ç‚¹": [node for node in dag.nodes() if in_degrees.get(node, 0) == 0],
            "å¶èŠ‚ç‚¹": [node for node in dag.nodes() if out_degrees.get(node, 0) == 0],
            "ä¸­ä»‹èŠ‚ç‚¹": [node for node in dag.nodes() if in_degrees.get(node, 0) > 0 and out_degrees.get(node, 0) > 0]
        }
    }
    
    json_file = os.path.join(output_folder, "ExpertInLoop_å› æžœç»“æžœ.json")
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    return txt_file, csv_file, graph_file, json_file, results

def run_expert_in_loop_algorithm():
    global LLM_MODEL, SELECT_PAIRS_THRESHOLD, MAX_LLM_PAIRS, LLM_ONLY
    """è¿è¡Œä¸“å®¶åœ¨å¾ªçŽ¯ç®—æ³•"""
    print("=" * 60)
    print("05 ä¸“å®¶åœ¨å¾ªçŽ¯ (Expert In The Loop) - å¼€å§‹æ‰§è¡Œ")
    print("ä½¿ç”¨LLMè¿›è¡Œæ™ºèƒ½å› æžœæŽ¨æ–­")
    print("=" * 60)
    try:
        print("æ¨¡åž‹é€‰æ‹©: 0:qwen2.5:7b  1:qwen2.5:32b  2:mannix/qwen2-57b:latest  3:huihui_ai/deepseek-r1-abliterated:70b  4:deepseek-r1:32b")
        s = input("è¯·é€‰æ‹©æ¨¡åž‹ç¼–å·(é»˜è®¤0): ").strip()
        idx = 0 if s == "" else int(s)
        if 0 <= idx < len(MODEL_CHOICES):
            LLM_MODEL = MODEL_CHOICES[idx]
            os.environ["LLM_MODEL"] = LLM_MODEL
            print(f"ä½¿ç”¨æ¨¡åž‹: {LLM_MODEL}")
        t = input(f"å€™é€‰ç­›é€‰é˜ˆå€¼(å½“å‰{SELECT_PAIRS_THRESHOLD}): ").strip()
        if t:
            try:
                v = float(t)
                SELECT_PAIRS_THRESHOLD = v
                os.environ["SELECT_PAIRS_THRESHOLD"] = str(v)
                print(f"è®¾å®šé˜ˆå€¼ä¸º: {SELECT_PAIRS_THRESHOLD}")
            except:
                pass
        m = input(f"å€™é€‰æ•°é‡ä¸Šé™(å½“å‰{MAX_LLM_PAIRS}): ").strip()
        if m:
            try:
                mv = int(m)
                MAX_LLM_PAIRS = mv
                os.environ["MAX_LLM_PAIRS"] = str(mv)
                print(f"è®¾å®šä¸Šé™ä¸º: {MAX_LLM_PAIRS}")
            except:
                pass
        yn = input(f"ä»…æ·»åŠ æ¥æºä¸ºLLMçš„è¾¹(å½“å‰{'æ˜¯' if LLM_ONLY else 'å¦'}) [Y/n]: ").strip().lower()
        if yn in ("", "y", "yes"):
            LLM_ONLY = True
            os.environ["LLM_ONLY"] = "1"
        elif yn in ("n", "no"):
            LLM_ONLY = False
            os.environ["LLM_ONLY"] = "0"
        print(f"ä»…æ·»åŠ LLMè¾¹: {'æ˜¯' if LLM_ONLY else 'å¦'}")
    except Exception:
        pass
    
    # 1. åŠ è½½æ•°æ®
    df = load_data()
    
    # 2. åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹
    output_dir = create_output_folder()
    
    # 3. æ•°æ®é¢„å¤„ç†
    df_processed = preprocess_data(df)
    
    # 4. åˆ›å»ºå˜é‡æè¿°
    variable_descriptions = create_variable_descriptions(df_processed)
    print(f"âœ“ åˆ›å»ºäº†{len(variable_descriptions)}ä¸ªå˜é‡çš„æè¿°")
    
    # 5. ä½¿ç”¨Expert-in-the-Loopè¿›è¡Œå› æžœå‘çŽ°
    print("ä½¿ç”¨Expert-in-the-Loopæ–¹æ³•ï¼Œç»“åˆLLMè¿›è¡Œè¾¹å®šå‘...")
    start_time = time.time()
    
    try:
        # åˆ›å»ºExpertInLoopä¼°è®¡å™¨
        precompute_orientations(df_processed, variable_descriptions)
        pg_utils.llm_pairwise_orient = fast_llm_pairwise_orient
        if FAST_MODE:
            from pgmpy.base import DAG
            dag = DAG()
            dag.add_nodes_from(df_processed.columns)
            processed = 0
            total_pairs = len(SELECTED_PAIRS)
            for u, v in SELECTED_PAIRS:
                k = (str(u), str(v)) if str(u) < str(v) else (str(v), str(u))
                o = ORIENT_CACHE.get(k)
                o = o if o is not None else fast_llm_pairwise_orient(u, v, variable_descriptions, LLM_MODEL)
                src = EDGE_SOURCE.get(k, "unknown")
                if LLM_ONLY and src != "llm":
                    processed += 1
                    if processed % 20 == 0 or processed == total_pairs:
                        print(f"æž„å›¾è¿›åº¦: å·²å¤„ç†å€™é€‰å¯¹ {processed}/{total_pairs}")
                    continue
                try:
                    dag.add_edge(o[0], o[1])
                except:
                    pass
                processed += 1
                if processed % 20 == 0 or processed == total_pairs:
                    print(f"æž„å›¾è¿›åº¦: å·²å¤„ç†å€™é€‰å¯¹ {processed}/{total_pairs}")
            keys = [(str(u), str(v)) if str(u) < str(v) else (str(v), str(u)) for u, v in SELECTED_PAIRS]
            c_llm = sum(1 for k in keys if EDGE_SOURCE.get(k) == "llm")
            c_rule = sum(1 for k in keys if EDGE_SOURCE.get(k) == "rule")
            c_fb = sum(1 for k in keys if EDGE_SOURCE.get(k) == "fallback")
            print(f"å®šå‘æ¥æº: LLM={c_llm} è§„åˆ™={c_rule} å›žé€€={c_fb}")
            learned_dag = dag
        else:
            estimator = ExpertInLoop(df_processed)
            learned_dag = estimator.estimate(
                pval_threshold=EIL_PVAL_THRESHOLD,
                effect_size_threshold=EIL_EFFECT_SIZE,
                variable_descriptions=variable_descriptions,
                llm_model=LLM_MODEL,
                use_cache=True,
                show_progress=True
            )
        
        if learned_dag is None:
            raise ValueError("ExpertInLoop.estimate() è¿”å›žäº† None")
        
        end_time = time.time()
        execution_time = end_time - start_time
        
        print(f"âœ“ ä¸“å®¶åœ¨å¾ªçŽ¯å®Œæˆï¼Œè€—æ—¶: {execution_time:.2f}ç§’")
        print(f"âœ“ å‘çŽ° {len(learned_dag.edges())} æ¡å› æžœè¾¹")
        
        # 6. ä¿å­˜ç»“æžœ
        txt_file, csv_file, graph_file, json_file, results = save_dag_results(learned_dag, output_dir, df_processed.columns)
        
        # 7. è¾“å‡ºç»“æžœæ‘˜è¦
        print("\n" + "=" * 60)
        print("ä¸“å®¶åœ¨å¾ªçŽ¯æ‰§è¡Œå®Œæˆ - ç»“æžœæ‘˜è¦")
        print("=" * 60)
        print(f"ç­–ç•¥: LLMæ™ºèƒ½å®šå‘")
        print(f"æ‰§è¡Œæ—¶é—´: {execution_time:.2f}ç§’")
        print(f"æ•°æ®ç»´åº¦: {df_processed.shape[0]} Ã— {df_processed.shape[1]}")
        print(f"å‘çŽ°çš„å› æžœè¾¹æ•°é‡: {results['ç½‘ç»œç»“æž„']['è¾¹æ€»æ•°']}")
        print(f"ç½‘ç»œèŠ‚ç‚¹æ•°é‡: {results['ç½‘ç»œç»“æž„']['èŠ‚ç‚¹æ€»æ•°']}")
        print(f"æ ¹èŠ‚ç‚¹æ•°é‡: {len(results['èŠ‚ç‚¹åˆ†æž']['æ ¹èŠ‚ç‚¹'])}")
        print(f"å¶èŠ‚ç‚¹æ•°é‡: {len(results['èŠ‚ç‚¹åˆ†æž']['å¶èŠ‚ç‚¹'])}")
        print(f"ä¸­ä»‹èŠ‚ç‚¹æ•°é‡: {len(results['èŠ‚ç‚¹åˆ†æž']['ä¸­ä»‹èŠ‚ç‚¹'])}")
        print(f"å¹³å‡èŠ‚ç‚¹åº¦æ•°: {results['ç»Ÿè®¡ä¿¡æ¯']['å¹³å‡åº¦æ•°']:.2f}")
        
        print(f"\nðŸ“ ç»“æžœä¿å­˜ä½ç½®:")
        print(f"  - TXTæ–‡ä»¶: {txt_file}")
        print(f"  - CSVæ–‡ä»¶: {csv_file}")
        print(f"  - ç½‘ç»œå›¾: {graph_file}")
        print(f"  - JSONç»“æžœ: {json_file}")
        
        return output_dir, len(learned_dag.edges())
        
    except Exception as e:
        print(f"âŒ ä¸“å®¶åœ¨å¾ªçŽ¯æ‰§è¡Œå¤±è´¥: {str(e)}")
        # ä½¿ç”¨å¿«é€Ÿå›žé€€ç­–ç•¥
        print("ä½¿ç”¨å¿«é€Ÿå›žé€€ç­–ç•¥...")
        from pgmpy.base import DAG
        
        dag = DAG()
        dag.add_nodes_from(df_processed.columns)
        
        # åŸºäºŽç›¸å…³æ€§æ·»åŠ è¾¹
        corr_matrix = df_processed.corr().abs()
        edges_added = 0
        max_edges = 50
        
        for i, col1 in enumerate(df_processed.columns):
            for j, col2 in enumerate(df_processed.columns):
                if i < j and edges_added < max_edges:
                    corr_val = corr_matrix.loc[col1, col2]
                    if corr_val >= 0.3:
                        try:
                            dag.add_edge(col1, col2)
                            edges_added += 1
                        except:
                            continue
        
        end_time = time.time()
        execution_time = end_time - start_time
        
        print(f"âœ“ å¿«é€Ÿå›žé€€å®Œæˆï¼Œè€—æ—¶: {execution_time:.2f}ç§’")
        print(f"âœ“ å‘çŽ° {len(dag.edges())} æ¡å› æžœè¾¹")
        
        txt_file, csv_file, graph_file, json_file, results = save_dag_results(dag, output_dir, df_processed.columns)
        
        return output_dir, len(dag.edges())

if __name__ == "__main__":
    import time
    try:
        output_dir, edge_count = run_expert_in_loop_algorithm()
        print(f"\nâœ… 05 ä¸“å®¶åœ¨å¾ªçŽ¯æ‰§è¡ŒæˆåŠŸï¼å‘çŽ° {edge_count} æ¡å› æžœè¾¹")
    except Exception as e:
        print(f"\nâŒ 05 ä¸“å®¶åœ¨å¾ªçŽ¯æ‰§è¡Œå¤±è´¥: {str(e)}")
        raise
```

## 06 å› æžœè¾¹ç­›é€‰ç®—æ³• (ä¸“ä¸šç‰ˆ)

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é«˜çº§å› æžœè¾¹ç­›é€‰ç®—æ³• - ä¸“ä¸šç‰ˆ
Advanced Causal Edge Selection Algorithm - Professional Edition

åŸºäºŽå¤šç»´åº¦è¯„ä¼°å’Œæœºå™¨å­¦ä¹ çš„æ™ºèƒ½å› æžœè¾¹ç­›é€‰ç³»ç»Ÿ
Multi-dimensional Assessment and Machine Learning-based Intelligent Causal Edge Selection System

æ ¸å¿ƒç‰¹æ€§ï¼š
1. ç®—æ³•ä¸€è‡´æ€§è¯„åˆ† - æ‰€æœ‰ç®—æ³•æƒé‡ç›¸ç­‰
2. å®Œæ•´ç½‘ç»œæ‹“æ‰‘è¯„åˆ† - åŒ…å«åº¦ä¸­å¿ƒæ€§ã€ä»‹æ•°ä¸­å¿ƒæ€§ã€æŽ¥è¿‘ä¸­å¿ƒæ€§
3. ç»Ÿè®¡æ˜¾è‘—æ€§è¯„åˆ† - åŸºäºŽé¢‘æ¬¡åˆ†å¸ƒçš„ç»Ÿè®¡æ£€éªŒ
4. è‡ªé€‚åº”é˜ˆå€¼é€‰æ‹© - è‚˜éƒ¨æ³•åˆ™ã€èšç±»åˆ†æžã€å¼‚å¸¸æ£€æµ‹
5. è´¨é‡åˆ†å±‚ç®¡ç† - é“‚é‡‘ã€é»„é‡‘ã€ç™½é“¶ã€é’é“œå››çº§è´¨é‡ä½“ç³»

ä½œè€…: å› æžœå‘çŽ°ç³»ç»Ÿ
ç‰ˆæœ¬: Professional Edition v2.0
æ—¥æœŸ: 2025å¹´
"""

import pandas as pd
import numpy as np
import os
import json
from datetime import datetime
from sklearn.ensemble import IsolationForest
from sklearn.cluster import DBSCAN
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score
import networkx as nx
import warnings
from collections import Counter, defaultdict

warnings.filterwarnings('ignore')

class ProfessionalCausalEdgeSelector:
    """ä¸“ä¸šçº§å› æžœè¾¹ç­›é€‰å™¨"""
    
    def __init__(self):
        self.scaler = StandardScaler()
        self.network_graph = None
        
    def load_algorithm_results(self):
        """ç›´æŽ¥åŠ è½½æ‰€æœ‰ç®—æ³•çš„å› æžœè¾¹ç»“æžœ"""
        script_dir = os.path.dirname(os.path.abspath(__file__))
        
        # å®šä¹‰ç®—æ³•ç»“æžœæ–‡ä»¶è·¯å¾„
        algorithm_configs = [
            {
                "name": "PCç®—æ³•",
                "folder": "01PCç®—æ³•ç»“æžœ",
                "csv_file": "PC_å› æžœè¾¹åˆ—è¡¨.csv"
            },
            {
                "name": "çˆ¬å±±ç®—æ³•",
                "folder": "02çˆ¬å±±ç®—æ³•ç»“æžœ",
                "csv_file": "HillClimbing_AIC-D_å› æžœè¾¹åˆ—è¡¨.csv"
            },
            {
                "name": "è´ªå©ªç­‰ä»·æœç´¢",
                "folder": "03è´ªå©ªç­‰ä»·æœç´¢ç»“æžœ",
                "csv_file": "GreedyEquivalence_AIC-D_å› æžœè¾¹åˆ—è¡¨.csv"
            },
            {
                "name": "æ ‘æœç´¢",
                "folder": "04æ ‘æœç´¢ç»“æžœ",
                "csv_file": "TAN_å› æžœè¾¹åˆ—è¡¨.csv"
            },
            {
                "name": "ä¸“å®¶åœ¨å¾ªçŽ¯",
                "folder": "05ä¸“å®¶åœ¨å¾ªçŽ¯ç»“æžœ",
                "csv_file": "ExpertInLoop_å› æžœè¾¹åˆ—è¡¨.csv"
            }
        ]
        
        all_edges = []
        algorithm_stats = {}
        
        print("æ­£åœ¨åŠ è½½å„ç®—æ³•çš„å› æžœè¾¹ç»“æžœ...")
        
        for config in algorithm_configs:
            try:
                # æž„å»ºæ–‡ä»¶è·¯å¾„
                folder_path = os.path.join(script_dir, config["folder"])
                csv_path = os.path.join(folder_path, config["csv_file"])
                
                if os.path.exists(csv_path):
                    # åŠ è½½CSVæ–‡ä»¶
                    df = pd.read_csv(csv_path, encoding='utf-8-sig')
                    
                    # æ ‡å‡†åŒ–åˆ—å
                    if len(df.columns) >= 2:
                        df.columns = ['æºèŠ‚ç‚¹', 'ç›®æ ‡èŠ‚ç‚¹']
                        
                        # æ·»åŠ ç®—æ³•æ ‡è¯†
                        df['ç®—æ³•'] = config["name"]
                        
                        # åˆ›å»ºè¾¹çš„å”¯ä¸€æ ‡è¯†
                        df['è¾¹æ ‡è¯†'] = df['æºèŠ‚ç‚¹'] + ' -> ' + df['ç›®æ ‡èŠ‚ç‚¹']
                        
                        all_edges.append(df)
                        
                        # è®°å½•ç»Ÿè®¡ä¿¡æ¯
                        algorithm_stats[config["name"]] = {
                            "è¾¹æ•°é‡": len(df),
                            "æ–‡ä»¶è·¯å¾„": csv_path,
                            "åŠ è½½çŠ¶æ€": "æˆåŠŸ"
                        }
                        
                        print(f"âœ“ {config['name']}: åŠ è½½ {len(df)} æ¡å› æžœè¾¹")
                    else:
                        print(f"âš  {config['name']}: CSVæ–‡ä»¶æ ¼å¼ä¸æ­£ç¡®")
                        algorithm_stats[config["name"]] = {
                            "è¾¹æ•°é‡": 0,
                            "æ–‡ä»¶è·¯å¾„": csv_path,
                            "åŠ è½½çŠ¶æ€": "æ ¼å¼é”™è¯¯"
                        }
                else:
                    print(f"âš  {config['name']}: æ–‡ä»¶ä¸å­˜åœ¨ - {csv_path}")
                    algorithm_stats[config["name"]] = {
                        "è¾¹æ•°é‡": 0,
                        "æ–‡ä»¶è·¯å¾„": csv_path,
                        "åŠ è½½çŠ¶æ€": "æ–‡ä»¶ä¸å­˜åœ¨"
                    }
                    
            except Exception as e:
                print(f"âŒ {config['name']}: åŠ è½½å¤±è´¥ - {str(e)}")
                algorithm_stats[config["name"]] = {
                    "è¾¹æ•°é‡": 0,
                    "æ–‡ä»¶è·¯å¾„": "N/A",
                    "åŠ è½½çŠ¶æ€": f"é”™è¯¯: {str(e)}"
                }
        
        return all_edges, algorithm_stats
    
    def merge_and_analyze_edges(self, all_edges):
        """åˆå¹¶å’Œåˆ†æžå› æžœè¾¹"""
        if not all_edges:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å› æžœè¾¹æ•°æ®")
            return None
        
        # åˆå¹¶æ‰€æœ‰è¾¹
        combined_df = pd.concat(all_edges, ignore_index=True)
        
        print(f"\næ€»å…±æ”¶é›†åˆ° {len(combined_df)} æ¡å› æžœè¾¹ï¼ˆåŒ…å«é‡å¤ï¼‰")
        
        # ç»Ÿè®¡æ¯æ¡è¾¹å‡ºçŽ°çš„é¢‘æ¬¡
        edge_counts = combined_df['è¾¹æ ‡è¯†'].value_counts()
        
        # åˆ›å»ºè¯¦ç»†çš„è¾¹åˆ†æž
        edge_analysis = []
        
        for edge_id, count in edge_counts.items():
            # èŽ·å–è¯¥è¾¹çš„æ‰€æœ‰è®°å½•
            edge_records = combined_df[combined_df['è¾¹æ ‡è¯†'] == edge_id]
            
            # æå–æºèŠ‚ç‚¹å’Œç›®æ ‡èŠ‚ç‚¹
            source = edge_records.iloc[0]['æºèŠ‚ç‚¹']
            target = edge_records.iloc[0]['ç›®æ ‡èŠ‚ç‚¹']
            
            # èŽ·å–æ”¯æŒè¯¥è¾¹çš„ç®—æ³•
            supporting_algorithms = edge_records['ç®—æ³•'].tolist()
            
            # è®¡ç®—ç»¼åˆè¯„åˆ†ï¼ˆåŸºäºŽå‡ºçŽ°é¢‘æ¬¡å’Œç®—æ³•å¤šæ ·æ€§ï¼‰
            frequency_score = count / len(all_edges)  # é¢‘æ¬¡è¯„åˆ†
            diversity_score = len(set(supporting_algorithms)) / 5  # ç®—æ³•å¤šæ ·æ€§è¯„åˆ†ï¼ˆæœ€å¤š5ç§ç®—æ³•ï¼‰
            comprehensive_score = (frequency_score * 0.6 + diversity_score * 0.4) * 100
            
            edge_analysis.append({
                'è¾¹æ ‡è¯†': edge_id,
                'æºèŠ‚ç‚¹': source,
                'ç›®æ ‡èŠ‚ç‚¹': target,
                'å‡ºçŽ°é¢‘æ¬¡': count,
                'æ”¯æŒç®—æ³•æ•°é‡': len(set(supporting_algorithms)),
                'æ”¯æŒç®—æ³•': ', '.join(set(supporting_algorithms)),
                'é¢‘æ¬¡è¯„åˆ†': frequency_score,
                'å¤šæ ·æ€§è¯„åˆ†': diversity_score,
                'ç»¼åˆè¯„åˆ†': comprehensive_score
            })
        
        # è½¬æ¢ä¸ºDataFrameå¹¶æŽ’åº
        edge_df = pd.DataFrame(edge_analysis)
        edge_df = edge_df.sort_values('ç»¼åˆè¯„åˆ†', ascending=False).reset_index(drop=True)
        
        print(f"å‘çŽ° {len(edge_df)} æ¡å”¯ä¸€å› æžœè¾¹")
        
        return edge_df
    
    def calculate_advanced_scores(self, edge_df):
        """è®¡ç®—é«˜çº§è¯„åˆ†æŒ‡æ ‡"""
        
        # 1. ç®—æ³•ä¸€è‡´æ€§è¯„åˆ†ï¼ˆæ‰€æœ‰ç®—æ³•æƒé‡ç›¸ç­‰ï¼‰
        algorithm_weights = {
            "PCç®—æ³•": 1.0,
            "çˆ¬å±±ç®—æ³•": 1.0,
            "è´ªå©ªç­‰ä»·æœç´¢": 1.0,
            "æ ‘æœç´¢": 1.0,
            "ä¸“å®¶åœ¨å¾ªçŽ¯": 1.0
        }
        
        consistency_scores = []
        for _, row in edge_df.iterrows():
            supporting_algs = row['æ”¯æŒç®—æ³•'].split(', ')
            weighted_support = sum(algorithm_weights.get(alg, 1.0) for alg in supporting_algs)
            max_possible = sum(algorithm_weights.values())
            consistency_scores.append(weighted_support / max_possible)
        
        edge_df['ç®—æ³•ä¸€è‡´æ€§è¯„åˆ†'] = consistency_scores
        
        # 2. ç½‘ç»œæ‹“æ‰‘è¯„åˆ†ï¼ˆå®Œæ•´ç‰ˆï¼‰
        topology_scores = self._calculate_comprehensive_topology_scores(edge_df)
        edge_df['ç½‘ç»œæ‹“æ‰‘è¯„åˆ†'] = topology_scores
        
        # 3. ç»Ÿè®¡æ˜¾è‘—æ€§è¯„åˆ†ï¼ˆåŸºäºŽé¢‘æ¬¡çš„ç»Ÿè®¡æ£€éªŒï¼‰
        significance_scores = self._calculate_significance_scores(edge_df)
        edge_df['ç»Ÿè®¡æ˜¾è‘—æ€§è¯„åˆ†'] = significance_scores
        
        return edge_df
    
    def _calculate_comprehensive_topology_scores(self, edge_df):
        """è®¡ç®—å®Œæ•´çš„ç½‘ç»œæ‹“æ‰‘è¯„åˆ†"""
        # æž„å»ºç½‘ç»œå›¾
        G = nx.DiGraph()
        
        for _, row in edge_df.iterrows():
            G.add_edge(row['æºèŠ‚ç‚¹'], row['ç›®æ ‡èŠ‚ç‚¹'])
        
        self.network_graph = G
        
        # è®¡ç®—å„ç§ä¸­å¿ƒæ€§æŒ‡æ ‡
        try:
            # åº¦ä¸­å¿ƒæ€§
            degree_centrality = nx.degree_centrality(G)
            
            # ä»‹æ•°ä¸­å¿ƒæ€§
            betweenness_centrality = nx.betweenness_centrality(G)
            
            # æŽ¥è¿‘ä¸­å¿ƒæ€§
            closeness_centrality = nx.closeness_centrality(G)
            
            # PageRankä¸­å¿ƒæ€§
            pagerank_centrality = nx.pagerank(G, alpha=0.85)
            
            # ç‰¹å¾å‘é‡ä¸­å¿ƒæ€§ï¼ˆå¯¹äºŽå¼ºè¿žé€šå›¾ï¼‰
            try:
                eigenvector_centrality = nx.eigenvector_centrality(G, max_iter=1000)
            except:
                eigenvector_centrality = {node: 0.5 for node in G.nodes()}
            
        except Exception as e:
            print(f"ç½‘ç»œæ‹“æ‰‘è®¡ç®—è­¦å‘Š: {e}")
            # å¦‚æžœè®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼
            nodes = list(G.nodes())
            degree_centrality = {node: 0.5 for node in nodes}
            betweenness_centrality = {node: 0.5 for node in nodes}
            closeness_centrality = {node: 0.5 for node in nodes}
            pagerank_centrality = {node: 0.5 for node in nodes}
            eigenvector_centrality = {node: 0.5 for node in nodes}
        
        topology_scores = []
        
        for _, row in edge_df.iterrows():
            source, target = row['æºèŠ‚ç‚¹'], row['ç›®æ ‡èŠ‚ç‚¹']
            
            # èŽ·å–å„ç§ä¸­å¿ƒæ€§æŒ‡æ ‡
            source_degree = degree_centrality.get(source, 0)
            target_degree = degree_centrality.get(target, 0)
            
            source_betweenness = betweenness_centrality.get(source, 0)
            target_betweenness = betweenness_centrality.get(target, 0)
            
            source_closeness = closeness_centrality.get(source, 0)
            target_closeness = closeness_centrality.get(target, 0)
            
            source_pagerank = pagerank_centrality.get(source, 0)
            target_pagerank = pagerank_centrality.get(target, 0)
            
            source_eigenvector = eigenvector_centrality.get(source, 0)
            target_eigenvector = eigenvector_centrality.get(target, 0)
            
            # è®¡ç®—ç»¼åˆæ‹“æ‰‘è¯„åˆ†
            # æƒé‡åˆ†é…ï¼šåº¦ä¸­å¿ƒæ€§(0.25) + ä»‹æ•°ä¸­å¿ƒæ€§(0.25) + æŽ¥è¿‘ä¸­å¿ƒæ€§(0.2) + PageRank(0.2) + ç‰¹å¾å‘é‡(0.1)
            topology_score = (
                (source_degree + target_degree) / 2 * 0.25 +
                (source_betweenness + target_betweenness) / 2 * 0.25 +
                (source_closeness + target_closeness) / 2 * 0.2 +
                (source_pagerank + target_pagerank) / 2 * 0.2 +
                (source_eigenvector + target_eigenvector) / 2 * 0.1
            )
            
            topology_scores.append(min(topology_score, 1.0))
        
        return topology_scores
    
    def _calculate_significance_scores(self, edge_df):
        """è®¡ç®—ç»Ÿè®¡æ˜¾è‘—æ€§è¯„åˆ†"""
        # åŸºäºŽé¢‘æ¬¡åˆ†å¸ƒçš„Z-score
        frequencies = edge_df['å‡ºçŽ°é¢‘æ¬¡'].values
        mean_freq = np.mean(frequencies)
        std_freq = np.std(frequencies) if np.std(frequencies) > 0 else 1
        
        significance_scores = []
        for freq in frequencies:
            z_score = (freq - mean_freq) / std_freq
            # å°†Z-scoreè½¬æ¢ä¸º0-1èŒƒå›´çš„è¯„åˆ†
            significance = 1 / (1 + np.exp(-z_score))  # Sigmoidå‡½æ•°
            significance_scores.append(significance)
        
        return significance_scores
    
    def ensemble_scoring(self, edge_df):
        """é›†æˆè¯„åˆ†ï¼ˆç§»é™¤åŒ»å­¦åˆç†æ€§è¯„åˆ†ï¼‰"""
        # æƒé‡é…ç½®ï¼ˆé‡æ–°åˆ†é…æƒé‡ï¼‰
        weights = {
            'é¢‘æ¬¡è¯„åˆ†': 0.30,           # å¢žåŠ é¢‘æ¬¡æƒé‡
            'å¤šæ ·æ€§è¯„åˆ†': 0.25,         # å¢žåŠ å¤šæ ·æ€§æƒé‡
            'ç®—æ³•ä¸€è‡´æ€§è¯„åˆ†': 0.25,     # å¢žåŠ ä¸€è‡´æ€§æƒé‡
            'ç½‘ç»œæ‹“æ‰‘è¯„åˆ†': 0.20,       # ç½‘ç»œæ‹“æ‰‘æƒé‡
        }
        
        # è®¡ç®—åŠ æƒå¹³å‡
        edge_df['é›†æˆè¯„åˆ†'] = (
            edge_df['é¢‘æ¬¡è¯„åˆ†'] * weights['é¢‘æ¬¡è¯„åˆ†'] +
            edge_df['å¤šæ ·æ€§è¯„åˆ†'] * weights['å¤šæ ·æ€§è¯„åˆ†'] +
            edge_df['ç®—æ³•ä¸€è‡´æ€§è¯„åˆ†'] * weights['ç®—æ³•ä¸€è‡´æ€§è¯„åˆ†'] +
            edge_df['ç½‘ç»œæ‹“æ‰‘è¯„åˆ†'] * weights['ç½‘ç»œæ‹“æ‰‘è¯„åˆ†']
        )
        
        # æŒ‰é›†æˆè¯„åˆ†æŽ’åº
        edge_df = edge_df.sort_values('é›†æˆè¯„åˆ†', ascending=False).reset_index(drop=True)
        
        return edge_df
    
    def adaptive_threshold_selection(self, edge_df):
        """è‡ªé€‚åº”é˜ˆå€¼é€‰æ‹©"""
        scores = edge_df['é›†æˆè¯„åˆ†'].values
        
        # æ–¹æ³•1: è‚˜éƒ¨æ³•åˆ™
        elbow_threshold = 0.6  # é»˜è®¤å€¼
        try:
            # ç®€åŒ–çš„è‚˜éƒ¨æ£€æµ‹
            sorted_scores = np.sort(scores)[::-1]
            diffs = np.diff(sorted_scores)
            if len(diffs) > 0:
                elbow_idx = np.argmax(diffs) + 1
                elbow_threshold = sorted_scores[elbow_idx] if elbow_idx < len(sorted_scores) else 0.6
        except:
            pass
        
        # æ–¹æ³•2: èšç±»åˆ†æž
        cluster_threshold = 0.65  # é»˜è®¤å€¼
        try:
            if len(scores) > 5:
                scores_reshaped = scores.reshape(-1, 1)
                scaler = StandardScaler()
                scores_scaled = scaler.fit_transform(scores_reshaped)
                
                dbscan = DBSCAN(eps=0.3, min_samples=2)
                clusters = dbscan.fit_predict(scores_scaled)
                
                if len(set(clusters)) > 1:
                    # æ‰¾åˆ°æœ€é«˜è´¨é‡çš„èšç±»
                    cluster_means = {}
                    for cluster_id in set(clusters):
                        if cluster_id != -1:  # æŽ’é™¤å™ªå£°ç‚¹
                            cluster_scores = scores[clusters == cluster_id]
                            cluster_means[cluster_id] = np.mean(cluster_scores)
                    
                    if cluster_means:
                        best_cluster = max(cluster_means.keys(), key=lambda x: cluster_means[x])
                        cluster_threshold = np.min(scores[clusters == best_cluster])
        except:
            pass
        
        # æ–¹æ³•3: å¼‚å¸¸æ£€æµ‹
        anomaly_threshold = 0.7  # é»˜è®¤å€¼
        try:
            if len(scores) > 10:
                isolation_forest = IsolationForest(contamination=0.3, random_state=42)
                scores_reshaped = scores.reshape(-1, 1)
                outliers = isolation_forest.fit_predict(scores_reshaped)
                
                # æ­£å¸¸ç‚¹çš„æœ€ä½Žåˆ†æ•°ä½œä¸ºé˜ˆå€¼
                normal_scores = scores[outliers == 1]
                if len(normal_scores) > 0:
                    anomaly_threshold = np.min(normal_scores)
        except:
            pass
        
        # ç»¼åˆé˜ˆå€¼
        final_threshold = np.mean([elbow_threshold, cluster_threshold, anomaly_threshold])
        
        return {
            "è‚˜éƒ¨æ³•åˆ™é˜ˆå€¼": float(elbow_threshold),
            "èšç±»åˆ†æžé˜ˆå€¼": float(cluster_threshold),
            "å¼‚å¸¸æ£€æµ‹é˜ˆå€¼": float(anomaly_threshold),
            "æœ€ç»ˆé˜ˆå€¼": float(final_threshold)
        }
    
    def quality_based_selection(self, edge_df, quality_tiers=None):
        """åŸºäºŽè´¨é‡åˆ†å±‚çš„é€‰æ‹©"""
        if quality_tiers is None:
            quality_tiers = {
                "platinum": {"min_score": 0.8, "min_algorithms": 4, "min_frequency": 3},
                "gold": {"min_score": 0.65, "min_algorithms": 3, "min_frequency": 2},
                "silver": {"min_score": 0.5, "min_algorithms": 2, "min_frequency": 2},
                "bronze": {"min_score": 0.3, "min_algorithms": 1, "min_frequency": 1}
            }
        
        selected_edges = []
        
        for _, row in edge_df.iterrows():
            score = row['é›†æˆè¯„åˆ†']
            algorithms = row['æ”¯æŒç®—æ³•æ•°é‡']
            frequency = row['å‡ºçŽ°é¢‘æ¬¡']
            
            quality_level = None
            
            # æŒ‰è´¨é‡ç­‰çº§ä»Žé«˜åˆ°ä½Žæ£€æŸ¥
            for level, criteria in quality_tiers.items():
                if (score >= criteria["min_score"] and 
                    algorithms >= criteria["min_algorithms"] and 
                    frequency >= criteria["min_frequency"]):
                    quality_level = level
                    break
            
            if quality_level:
                row_dict = row.to_dict()
                row_dict['è´¨é‡ç­‰çº§'] = quality_level
                selected_edges.append(row_dict)
        
        return pd.DataFrame(selected_edges) if selected_edges else pd.DataFrame()

def run_professional_causal_edge_selection():
    """è¿è¡Œä¸“ä¸šçº§å› æžœè¾¹ç­›é€‰ç®—æ³•"""
    print("=" * 80)
    print("é«˜çº§å› æžœè¾¹ç­›é€‰ç®—æ³• - ä¸“ä¸šç‰ˆ - Professional Edition")
    print("Advanced Causal Edge Selection Algorithm - Professional Edition")
    print("=" * 80)
    
    # 1. åˆå§‹åŒ–ç­›é€‰å™¨
    selector = ProfessionalCausalEdgeSelector()
    
    # 2. ç›´æŽ¥åŠ è½½ç®—æ³•ç»“æžœ
    all_edges, algorithm_stats = selector.load_algorithm_results()
    
    if not all_edges:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•ç®—æ³•ç»“æžœï¼Œè¯·å…ˆè¿è¡Œ01-05ç®—æ³•è„šæœ¬")
        return None
    
    # 3. åˆå¹¶å’Œåˆ†æžè¾¹
    edge_df = selector.merge_and_analyze_edges(all_edges)
    
    if edge_df is None:
        print("âŒ è¾¹åˆ†æžå¤±è´¥")
        return None
    
    # 4. è®¡ç®—é«˜çº§è¯„åˆ†
    print("\næ­£åœ¨è®¡ç®—é«˜çº§è¯„åˆ†æŒ‡æ ‡...")
    edge_df = selector.calculate_advanced_scores(edge_df)
    
    # 5. é›†æˆè¯„åˆ†
    print("æ­£åœ¨è®¡ç®—é›†æˆè¯„åˆ†...")
    edge_df = selector.ensemble_scoring(edge_df)
    
    # 6. è‡ªé€‚åº”é˜ˆå€¼é€‰æ‹©
    print("æ­£åœ¨è¿›è¡Œè‡ªé€‚åº”é˜ˆå€¼é€‰æ‹©...")
    thresholds = selector.adaptive_threshold_selection(edge_df)
    
    # 7. åŸºäºŽè´¨é‡åˆ†å±‚çš„é€‰æ‹©
    print("æ­£åœ¨è¿›è¡Œè´¨é‡åˆ†å±‚é€‰æ‹©...")
    selected_edges = selector.quality_based_selection(edge_df)
    
    # 8. ä¿å­˜ç»“æžœï¼ˆä½¿ç”¨ä¸“ä¸šå‘½åï¼‰
    script_dir = os.path.dirname(os.path.abspath(__file__))
    output_dir = os.path.join(script_dir, "06å€™é€‰å› æžœè¾¹é›†åˆ")
    os.makedirs(output_dir, exist_ok=True)
    
    # ä¿å­˜å®Œæ•´è¯„åˆ†ç»“æžœ
    comprehensive_results_file = 'å› æžœè¾¹ç»¼åˆè¯„åˆ†ç»“æžœ.csv'
    edge_df.to_csv(os.path.join(output_dir, comprehensive_results_file), 
                   index=False, encoding='utf-8-sig')
    
    # ä¿å­˜ç­›é€‰ç»“æžœ
    if len(selected_edges) > 0:
        # é«˜è´¨é‡å› æžœè¾¹å€™é€‰é›†
        high_quality_edges_file = 'é«˜è´¨é‡å› æžœè¾¹å€™é€‰é›†.csv'
        selected_edges.to_csv(os.path.join(output_dir, high_quality_edges_file), 
                             index=False, encoding='utf-8-sig')
        
        # ç²¾ç®€ç‰ˆå› æžœè¾¹åˆ—è¡¨
        simplified_edges = selected_edges[['æºèŠ‚ç‚¹', 'ç›®æ ‡èŠ‚ç‚¹', 'è´¨é‡ç­‰çº§', 'é›†æˆè¯„åˆ†', 'æ”¯æŒç®—æ³•æ•°é‡']].copy()
        simplified_edges_file = 'ç²¾ç®€å› æžœè¾¹åˆ—è¡¨.csv'
        simplified_edges.to_csv(os.path.join(output_dir, simplified_edges_file), 
                               index=False, encoding='utf-8-sig')
    
    # ä¿å­˜åˆ†æžæŠ¥å‘Š
    report = {
        "ç­›é€‰ä¿¡æ¯": {
            "ç”Ÿæˆæ—¶é—´": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "ç®—æ³•ç‰ˆæœ¬": "Professional Edition v2.0",
            "è¾“å…¥è¾¹æ•°é‡": len(edge_df),
            "ç­›é€‰åŽè¾¹æ•°é‡": len(selected_edges),
            "ç­›é€‰çŽ‡": f"{len(selected_edges)/len(edge_df)*100:.2f}%"
        },
        "ç®—æ³•ç»Ÿè®¡": algorithm_stats,
        "é˜ˆå€¼åˆ†æž": thresholds,
        "è´¨é‡ç­‰çº§åˆ†å¸ƒ": {
            level: len(selected_edges[selected_edges['è´¨é‡ç­‰çº§'] == level])
            for level in ['platinum', 'gold', 'silver', 'bronze']
            if level in selected_edges['è´¨é‡ç­‰çº§'].values
        } if len(selected_edges) > 0 else {},
        "é›†æˆè¯„åˆ†ç»Ÿè®¡": {
            "æœ€é«˜åˆ†": float(edge_df['é›†æˆè¯„åˆ†'].max()),
            "æœ€ä½Žåˆ†": float(edge_df['é›†æˆè¯„åˆ†'].min()),
            "å¹³å‡åˆ†": float(edge_df['é›†æˆè¯„åˆ†'].mean()),
            "æ ‡å‡†å·®": float(edge_df['é›†æˆè¯„åˆ†'].std())
        },
        "ç½‘ç»œæ‹“æ‰‘ç»Ÿè®¡": {
            "èŠ‚ç‚¹æ•°é‡": selector.network_graph.number_of_nodes() if selector.network_graph else 0,
            "è¾¹æ•°é‡": selector.network_graph.number_of_edges() if selector.network_graph else 0,
            "å¹³å‡åº¦": float(np.mean([d for n, d in selector.network_graph.degree()])) if selector.network_graph and selector.network_graph.nodes() else 0
        }
    }
    # ä¿å­˜åˆ†æžæŠ¥å‘Š
    analysis_report_file = 'å› æžœè¾¹ç­›é€‰åˆ†æžæŠ¥å‘Š.json'
    with open(os.path.join(output_dir, analysis_report_file), 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    # 9. è¾“å‡ºç»“æžœæ‘˜è¦
    print("\n" + "=" * 60)
    print("ç­›é€‰ç»“æžœæ‘˜è¦ | Selection Results Summary")
    print("=" * 60)
    print(f"è¾“å…¥è¾¹æ•°é‡: {len(edge_df)}")
    print(f"ç­›é€‰åŽè¾¹æ•°é‡: {len(selected_edges)}")
    print(f"ç­›é€‰çŽ‡: {len(selected_edges)/len(edge_df)*100:.2f}%")
    
    if len(selected_edges) > 0:
        print("\nè´¨é‡ç­‰çº§åˆ†å¸ƒ:")
        quality_dist = selected_edges['è´¨é‡ç­‰çº§'].value_counts()
        for level in ['platinum', 'gold', 'silver', 'bronze']:
            if level in quality_dist.index:
                print(f"  {level.capitalize()}: {quality_dist[level]} æ¡è¾¹")
        
        print(f"\nå‰5æ¡é«˜è´¨é‡å› æžœè¾¹:")
        top_edges = selected_edges.head(5)
        for idx, row in top_edges.iterrows():
            print(f"  {idx+1}. {row['è¾¹æ ‡è¯†']} (è¯„åˆ†: {row['é›†æˆè¯„åˆ†']:.3f}, ç­‰çº§: {row['è´¨é‡ç­‰çº§']})")
    
    print(f"\nç”Ÿæˆæ–‡ä»¶:")
    print(f"  - å®Œæ•´è¯„åˆ†ç»“æžœ: {comprehensive_results_file}")
    if len(selected_edges) > 0:
        print(f"  - é«˜è´¨é‡å€™é€‰è¾¹: {high_quality_edges_file}")
        print(f"  - ç²¾ç®€è¾¹åˆ—è¡¨: {simplified_edges_file}")
    print(f"  - åˆ†æžæŠ¥å‘Š: {analysis_report_file}")
    
    print("\nâœ… ä¸“ä¸šçº§å› æžœè¾¹ç­›é€‰å®Œæˆ!")
    
    return {
        "edge_df": edge_df,
        "selected_edges": selected_edges,
        "thresholds": thresholds,
        "report": report
    }

if __name__ == "__main__":
    run_professional_causal_edge_selection()
```

